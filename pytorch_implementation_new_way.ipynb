{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"3D_model\",\n",
    "    \"output_dir\": \"trained_models\",\n",
    "    \n",
    "    \"input_mode\": \"av\",\n",
    "    \n",
    "    \"data_folder\": \"D:/something-something-project/data/videos/20bn-something-something-v2/\",\n",
    "    \"json_data_train\": \"D:/something-something-project/data/something-something-v2-train.json\",\n",
    "    \"json_data_val\": \"D:/something-something-project/data/something-something-v2-validation.json\",\n",
    "    \"json_data_test\": \"D:/something-something-project/data/something-something-v2-test.json\",\n",
    "    \n",
    "    \"json_data_labels\": \"D:/something-something-project/data/something-something-v2-mylabels.json\",\n",
    "    \n",
    "    \"num_workers\": 0,                      # for parallel processing\n",
    "    \n",
    "    \"num_classes\": 5,                    # number of classes to classify\n",
    "    \"batch_size\": 30,\n",
    "    \"clip_size\": 30,\n",
    "    \n",
    "    \"nclips_train\": 1,\n",
    "    \"nclips_val\": 1,\n",
    "    \n",
    "    \"upscale_factor_train\": 1.4,\n",
    "    \"upscale_factor_eval\": 1.0,\n",
    "    \n",
    "    \"step_size_train\": 1,\n",
    "    \"step_size_val\": 1,\n",
    "    \n",
    "    \"lr\": 0.008,\n",
    "    \"last_lr\": 0.00001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.00001,\n",
    "    \"num_epochs\": 30,\n",
    "    \"print_freq\": 1,\n",
    "    \n",
    "    \"conv_model\": \"models.model3d_1\",\n",
    "    \"input_spatial_size\": 84,\n",
    "    \n",
    "    \"column_units\": 512,\n",
    "    \"save_features\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>template</th>\n",
       "      <th>placeholders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78687</td>\n",
       "      <td>holding potato next to vicks vaporub bottle</td>\n",
       "      <td>Holding [something] next to [something]</td>\n",
       "      <td>[potato, vicks vaporub bottle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42326</td>\n",
       "      <td>spreading margarine onto bread</td>\n",
       "      <td>Spreading [something] onto [something]</td>\n",
       "      <td>[margarine, bread]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100904</td>\n",
       "      <td>putting pen on a surface</td>\n",
       "      <td>Putting [something] on a surface</td>\n",
       "      <td>[pen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80715</td>\n",
       "      <td>lifting up one end of bottle, then letting it ...</td>\n",
       "      <td>Lifting up one end of [something], then lettin...</td>\n",
       "      <td>[bottle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34899</td>\n",
       "      <td>holding bulb</td>\n",
       "      <td>Holding [something]</td>\n",
       "      <td>[bulb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              label  \\\n",
       "0   78687        holding potato next to vicks vaporub bottle   \n",
       "1   42326                     spreading margarine onto bread   \n",
       "2  100904                           putting pen on a surface   \n",
       "3   80715  lifting up one end of bottle, then letting it ...   \n",
       "4   34899                                       holding bulb   \n",
       "\n",
       "                                            template  \\\n",
       "0            Holding [something] next to [something]   \n",
       "1             Spreading [something] onto [something]   \n",
       "2                   Putting [something] on a surface   \n",
       "3  Lifting up one end of [something], then lettin...   \n",
       "4                                Holding [something]   \n",
       "\n",
       "                     placeholders  \n",
       "0  [potato, vicks vaporub bottle]  \n",
       "1              [margarine, bread]  \n",
       "2                           [pen]  \n",
       "3                        [bottle]  \n",
       "4                          [bulb]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = \"D:/something-something-project/data/something-something-v2-train.json\"\n",
    "\n",
    "train_data = pd.read_json(train, orient=\"record\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168913, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'Dropping [something]' : [], \n",
    "           'Holding [something]' : [],\n",
    "           'Moving [something]' : [], \n",
    "           'Picking [something]' : [],\n",
    "           'Poking [something]' : [],\n",
    "           'Pouring [something]' : [],\n",
    "           'Putting [something]' : [],\n",
    "           'Showing [something]' : [], \n",
    "           'Tearing [something]' : []}\n",
    "\n",
    "def get_class_name(template_name):\n",
    "    for key in classes:\n",
    "        if key in template_name:\n",
    "            return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    row_class = get_class_name(row['template'])\n",
    "    if (row_class != None):\n",
    "        # print ('Template: ', row['template'], '\\tclass: ', row_class)\n",
    "        classes[row_class].append((row['id'], row['placeholders'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping [something] :  4680\n",
      "Holding [something] :  6894\n",
      "Moving [something] :  14877\n",
      "Picking [something] :  980\n",
      "Poking [something] :  4194\n",
      "Pouring [something] :  1700\n",
      "Putting [something] :  16731\n",
      "Showing [something] :  4141\n",
      "Tearing [something] :  3021\n"
     ]
    }
   ],
   "source": [
    "for key in classes:\n",
    "    print (key, \": \", len(classes[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and read json data and construct a list containing video sample\n",
    "# (name, id, label, path)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListData = namedtuple('ListData', ['id', 'label', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_use = [\"Dropping something\",\n",
    "                  \"Holding something\",\n",
    "                  \"Poking something\",\n",
    "                  \"Showing something\",\n",
    "                  \"Tearing something\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a class to read json labels from <...>labels.json file provided in the dataset\n",
    "class BaseDataset:\n",
    "    \"Read json data and construct a list containing video sample ids, label and path\"\n",
    "    def __init__(self, json_input_path, json_path_labels, data_root, extension, is_test=False):\n",
    "        self.json_input_path = json_input_path\n",
    "        self.json_path_labels = json_path_labels\n",
    "        self.data_root = data_root\n",
    "        self.extension = extension\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.classes = self.read_json_labels()\n",
    "        self.classes_dict = self.get_two_way_dict(self.classes)\n",
    "        self.json_data = self.read_json_input()\n",
    "        \n",
    "    def display_data_members(self):\n",
    "        print (\"self.classes: \", type(self.classes), len(self.classes), self.classes)\n",
    "        print (\"self.classes_dict: \", type(self.classes_dict), len(self.classes_dict), self.classes_dict)\n",
    "        print (\"self.json_data: \", type(self.json_data), len(self.json_data))\n",
    "        for l in self.json_data[:5]:\n",
    "            print (l)\n",
    "    \n",
    "    def read_json_labels(self):\n",
    "        classes = []\n",
    "        with open(self.json_path_labels, 'rb') as jsonfile:\n",
    "            json_reader = json.load(jsonfile)\n",
    "            for elem in json_reader:\n",
    "                classes.append(elem)\n",
    "        return sorted(classes)\n",
    "    \n",
    "    def clean_template(self, template):\n",
    "        \"\"\"Replaces instances of '[something] --> 'something' \"\"\"\n",
    "        template = template.replace(\"[\", \"\")\n",
    "        template = template.replace(\"]\", \"\")\n",
    "        return template\n",
    "    \n",
    "    def get_two_way_dict(self, classes):\n",
    "        classes_dict = {}\n",
    "        for i, item in enumerate(classes):\n",
    "            classes_dict[item] = i\n",
    "            classes_dict[i] = item\n",
    "        return classes_dict\n",
    "    \n",
    "    def read_json_input(self):\n",
    "        json_data = []\n",
    "        if not self.is_test:\n",
    "            with open(self.json_input_path, 'rb') as jsonfile:\n",
    "                json_reader = json.load(jsonfile)\n",
    "                for elem in json_reader:\n",
    "                    label = self.clean_template(elem['template'])\n",
    "                    if label not in self.classes:\n",
    "                        continue\n",
    "                        raise ValueError(\"Label mismatch! Please correct\")\n",
    "                    item = ListData(elem['id'], label, os.path.join(self.data_root + elem['id'] + self.extension))\n",
    "                    json_data.append(item)\n",
    "        else:\n",
    "            with open(self.json_input_path, 'rb') as jsonfile:\n",
    "                json_reader = json.load(jsonfile)\n",
    "                for elem in json_reader:\n",
    "                    # add a dummy label for all test samples\n",
    "                    item = ListData(elem['id'],\n",
    "                                    \"Holding something\",\n",
    "                                    os.path.join(self.data_root,\n",
    "                                                 elem['id'] + self.extension)\n",
    "                                    )\n",
    "                    json_data.append(item)\n",
    "        return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining class specific to webm video format and inherit the base class\n",
    "class WebMDataset(BaseDataset):\n",
    "    def __init__(self, json_input_path, json_path_labels, data_root, is_test=False):\n",
    "        EXTENSION = \".webm\"\n",
    "        super().__init__(json_input_path, json_path_labels, data_root, EXTENSION, is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.classes:  <class 'list'> 25 ['Dropping something behind something', 'Dropping something in front of something', 'Dropping something into something', 'Dropping something next to something', 'Dropping something onto something', 'Holding something', 'Holding something behind something', 'Holding something in front of something', 'Holding something next to something', 'Holding something over something', 'Poking a hole into some substance', 'Poking a hole into something soft', 'Poking a stack of something so the stack collapses', 'Poking a stack of something without the stack collapsing', 'Poking something so it slightly moves', \"Poking something so lightly that it doesn't or almost doesn't move\", 'Poking something so that it falls over', 'Poking something so that it spins around', 'Showing a photo of something to the camera', 'Showing something behind something', 'Showing something next to something', 'Showing something on top of something', 'Showing something to the camera', 'Showing that something is empty', 'Showing that something is inside something']\n",
      "self.classes_dict:  <class 'dict'> 50 {'Dropping something behind something': 0, 0: 'Dropping something behind something', 'Dropping something in front of something': 1, 1: 'Dropping something in front of something', 'Dropping something into something': 2, 2: 'Dropping something into something', 'Dropping something next to something': 3, 3: 'Dropping something next to something', 'Dropping something onto something': 4, 4: 'Dropping something onto something', 'Holding something': 5, 5: 'Holding something', 'Holding something behind something': 6, 6: 'Holding something behind something', 'Holding something in front of something': 7, 7: 'Holding something in front of something', 'Holding something next to something': 8, 8: 'Holding something next to something', 'Holding something over something': 9, 9: 'Holding something over something', 'Poking a hole into some substance': 10, 10: 'Poking a hole into some substance', 'Poking a hole into something soft': 11, 11: 'Poking a hole into something soft', 'Poking a stack of something so the stack collapses': 12, 12: 'Poking a stack of something so the stack collapses', 'Poking a stack of something without the stack collapsing': 13, 13: 'Poking a stack of something without the stack collapsing', 'Poking something so it slightly moves': 14, 14: 'Poking something so it slightly moves', \"Poking something so lightly that it doesn't or almost doesn't move\": 15, 15: \"Poking something so lightly that it doesn't or almost doesn't move\", 'Poking something so that it falls over': 16, 16: 'Poking something so that it falls over', 'Poking something so that it spins around': 17, 17: 'Poking something so that it spins around', 'Showing a photo of something to the camera': 18, 18: 'Showing a photo of something to the camera', 'Showing something behind something': 19, 19: 'Showing something behind something', 'Showing something next to something': 20, 20: 'Showing something next to something', 'Showing something on top of something': 21, 21: 'Showing something on top of something', 'Showing something to the camera': 22, 22: 'Showing something to the camera', 'Showing that something is empty': 23, 23: 'Showing that something is empty', 'Showing that something is inside something': 24, 24: 'Showing that something is inside something'}\n",
      "self.json_data:  <class 'list'> 24381\n",
      "ListData(id='78687', label='Holding something next to something', path='D:/something-something-project/data/videos/20bn-something-something-v2/78687.webm')\n",
      "ListData(id='34899', label='Holding something', path='D:/something-something-project/data/videos/20bn-something-something-v2/34899.webm')\n",
      "ListData(id='82056', label='Holding something next to something', path='D:/something-something-project/data/videos/20bn-something-something-v2/82056.webm')\n",
      "ListData(id='134982', label='Dropping something in front of something', path='D:/something-something-project/data/videos/20bn-something-something-v2/134982.webm')\n",
      "ListData(id='170911', label='Showing that something is empty', path='D:/something-something-project/data/videos/20bn-something-something-v2/170911.webm')\n"
     ]
    }
   ],
   "source": [
    "# testing the read json class declared above and seeing how the data looks like\n",
    "webmdataset = WebMDataset(\"D:/something-something-project/data/something-something-v2-train.json\",\n",
    "                        \"D:/something-something-project/data/something-something-v2-mylabels.json\",\n",
    "                        \"D:/something-something-project/data/videos/20bn-something-something-v2/\")\n",
    "webmdataset.display_data_members()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, we have created a named tuple (name, id, location) format for all the train data\n",
    "# So, we just need to pass the train/validation/test.json file and we will get this list of named tuples for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import numbers\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming video\n",
    "\n",
    "# we will define a class which composes several transformations together\n",
    "class ComposeMix:\n",
    "    \"\"\"\n",
    "    Composes several transformations together. It takes a list of transformations,\n",
    "    where each element of transform is a list with two elemts.\n",
    "    First being the transformation function itself, second being a string indicating \"img\" or \"vid\" transform\n",
    "    \n",
    "    Args:\n",
    "        transforms (List[Transform, \"<type>\"]): list of transforms to compose. <type> = \"img\" | \"vid\"\n",
    "        \n",
    "    Example:\n",
    "        >>> transforms.ComposeMix([\n",
    "                                    [RandomCropVideo(84), \"vid\"],\n",
    "                                    [torchvision.transforms.ToTensor(), \"img\"],\n",
    "                                    [torchvision.transforms.Normalize(\n",
    "                                                                        mean=[0.485, 0.456, 0.406], # default values for imagenet\n",
    "                                                                        std=[0.229, 0.224, 0.225]\n",
    "                                                                    ), \"img\"]\n",
    "                                ])\n",
    "    As you can see, we first randomly crop a video for 84x84 pixels, \n",
    "    then convert the cropped image into tensor and finally normalize the image using default values for imagenet.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    \"\"\"\n",
    "    What is __call__() method?\n",
    "    class Example: \n",
    "        def __init__(self): \n",
    "            print(\"Instance Created\") \n",
    "      \n",
    "        # Defining __call__ method \n",
    "        def __call__(self): \n",
    "            print(\"Instance is called via special method\") \n",
    "  \n",
    "    # Instance created \n",
    "    e = Example() \n",
    "  \n",
    "    # __call__ method will be called \n",
    "    e()\n",
    "    \n",
    "    Output:\n",
    "    Instance Created\n",
    "    Instance is called via special method\n",
    "    \"\"\"\n",
    "    def __call__(self, imgs):\n",
    "        for t in self.transforms:\n",
    "            if t[1] == \"img\":\n",
    "                for idx, img in enumerate(imgs):\n",
    "                    imgs[idx] = t[0](img)\n",
    "            elif t[1] == \"vid\":\n",
    "                imgs = t[0](imgs)\n",
    "            else:\n",
    "                print (\"Please specify the transform type\")\n",
    "                raise ValueError\n",
    "        return imgs\n",
    "\n",
    "class RandomCropVideo:\n",
    "    \"\"\"\n",
    "    Crop the given video frames at a random location. Crop location is the same for all the frames.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an int instead of a sequence like\n",
    "            (w, h), a square crop (size, size) is made\n",
    "        padding: (cv2 constant): Method to be used for padding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, padding=0, pad_method=cv2.BORDER_CONSTANT):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        \n",
    "        self.padding = padding\n",
    "        self.pad_method = pad_method\n",
    "    \n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (numpy.array): Video to be cropped\n",
    "            Returns:\n",
    "                numpy.array: Cropped Video\n",
    "        \"\"\"\n",
    "        th, tw = self.size\n",
    "        h, w = imgs[0].shape[:2]\n",
    "        # Return random integers from low (inclusive) to high (exclusive)\n",
    "        x1 = np.random.randint(0, w - tw)\n",
    "        y1 = np.random.randint(0, h - th)\n",
    "        \n",
    "        for idx, img in enumerate(imgs):\n",
    "            if self.padding > 0:\n",
    "                img = cv2.copyMakeBorder(img, self.padding, self.padding,\n",
    "                                         self.padding, self.padding,\n",
    "                                         self.pad_method)\n",
    "            # sample crop locations if not given\n",
    "            # it is necessary to keep cropping same in a video\n",
    "            img_crop = img[y1:y1 + th, x1:x1 + tw]\n",
    "            imgs[idx] = img_crop\n",
    "        return imgs\n",
    "\n",
    "class RandomHorizontalFlipVideo:\n",
    "    \"\"\"Horizontally flip the given video frames randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            imgs (numpy.array): Video to be flipped.\n",
    "        Returns:\n",
    "            numpy.array: Randomly flipped video.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            for idx, img in enumerate(imgs):\n",
    "                imgs[idx] = cv2.flip(img, 1)\n",
    "        return imgs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class RandomReverseTimeVideo(object):\n",
    "    \"\"\"Reverse the given video frames in time randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            imgs (numpy.array): Video to be flipped.\n",
    "        Returns:\n",
    "            numpy.array: Randomly flipped video.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            imgs = imgs[::-1]\n",
    "        return imgs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class RandomRotationVideo(object):\n",
    "    \"\"\"Rotate the given video frames randomly with a given degree.\n",
    "    Args:\n",
    "        degree (float): degrees used to rotate the video\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree=10):\n",
    "        self.degree = degree\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            imgs (numpy.array): Video to be rotated.\n",
    "        Returns:\n",
    "            numpy.array: Randomly rotated video.\n",
    "        \"\"\"\n",
    "        h, w = imgs[0].shape[:2]\n",
    "        degree_sampled = np.random.choice(\n",
    "                            np.arange(-self.degree, self.degree, 0.5))\n",
    "        M = cv2.getRotationMatrix2D((w / 2, h / 2), degree_sampled, 1)\n",
    "\n",
    "        for idx, img in enumerate(imgs):\n",
    "            imgs[idx] = cv2.warpAffine(img, M, (w, h))\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(degree={})'.format(self.degree_sampled)\n",
    "\n",
    "\n",
    "class IdentityTransform(object):\n",
    "    \"\"\"\n",
    "    Returns same video back\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        return imgs\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    r\"\"\"Rescale the input image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``cv2.INTER_LINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=cv2.INTER_LINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(\n",
    "            size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (numpy.array): Image to be scaled.\n",
    "        Returns:\n",
    "            numpy.array: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            h, w = img.shape[:2]\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                if ow < w:\n",
    "                    return cv2.resize(img, (ow, oh), cv2.INTER_AREA)\n",
    "                else:\n",
    "                    return cv2.resize(img, (ow, oh))\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                if oh < h:\n",
    "                    return cv2.resize(img, (ow, oh), cv2.INTER_AREA)\n",
    "                else:\n",
    "                    return cv2.resize(img, (ow, oh))\n",
    "        else:\n",
    "            return cv2.resize(img, tuple(self.size))\n",
    "\n",
    "\n",
    "class UnNormalize(object):\n",
    "    \"\"\"Unnormalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel x std) + mean\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = np.array(mean).astype('float32')\n",
    "        self.std = np.array(std).astype('float32')\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            self.mean = torch.FloatTensor(self.mean)\n",
    "            self.std = torch.FloatTensor(self.std)\n",
    "\n",
    "            if (self.std.dim() != tensor.dim() or\n",
    "                    self.mean.dim() != tensor.dim()):\n",
    "                for i in range(tensor.dim() - self.std.dim()):\n",
    "                    self.std = self.std.unsqueeze(-1)\n",
    "                    self.mean = self.mean.unsqueeze(-1)\n",
    "\n",
    "            tensor = torch.add(torch.mul(tensor, self.std), self.mean)\n",
    "        else:\n",
    "            # Relying on Numpy broadcasting abilities\n",
    "            tensor = tensor * self.std + self.mean\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentor\n",
    "class Augmentor:\n",
    "    def __init__(self, augmentation_mappings_json=None,\n",
    "                augmentation_types_todo=None,\n",
    "                fps_jitter_factors=[1, 0.75, 0.5]):\n",
    "        self.augmentation_mappings_json = augmentation_mappings_json\n",
    "        self.augmentation_types_todo = augmentation_types_todo\n",
    "        self.fps_jitter_factors = fps_jitter_factors\n",
    "\n",
    "        # read json to get the mapping dict\n",
    "        self.augmentation_mapping = self.read_augmentation_mapping(\n",
    "                                        self.augmentation_mappings_json)\n",
    "        self.augmentation_transforms = self.define_augmentation_transforms()\n",
    "        \n",
    "    def __call__(self, imgs, label):\n",
    "        if not self.augmentation_mapping:\n",
    "            return imgs, label\n",
    "        else:\n",
    "            candidate_augmentations = {\"same\": label}\n",
    "            for candidate in self.augmentation_types_todo:\n",
    "                if candidate == \"jitter_fps\":\n",
    "                    continue\n",
    "                if label in self.augmentation_mapping[candidate]:\n",
    "                    if isinstance(self.augmentation_mapping[candidate], list):\n",
    "                        candidate_augmentations[candidate] = label\n",
    "                    elif isinstance(self.augmentation_mapping[candidate], dict):\n",
    "                        candidate_augmentations[candidate] = self.augmentation_mapping[candidate][label]\n",
    "                    else:\n",
    "                        print(\"Something wrong with data type specified in \"\n",
    "                              \"augmentation file. Please check!\")\n",
    "            augmentation_chosen = np.random.choice(list(candidate_augmentations.keys()))\n",
    "            imgs = self.augmentation_transforms[augmentation_chosen](imgs)\n",
    "            label = candidate_augmentations[augmentation_chosen]\n",
    "\n",
    "            return imgs, label\n",
    "        \n",
    "    def read_augmentation_mapping(self, path):\n",
    "        if path:\n",
    "            with open(path, \"rb\") as fp:\n",
    "                mapping = json.load(fp)\n",
    "        else:\n",
    "            mapping = None\n",
    "        return mapping\n",
    "\n",
    "    def define_augmentation_transforms(self, ):\n",
    "        augmentation_transforms = {}\n",
    "        augmentation_transforms[\"same\"] = IdentityTransform()\n",
    "        augmentation_transforms[\"left/right\"] = RandomHorizontalFlipVideo(1)\n",
    "        augmentation_transforms[\"left/right agnostic\"] = RandomHorizontalFlipVideo(1)\n",
    "        augmentation_transforms[\"reverse time\"] = RandomReverseTimeVideo(1)\n",
    "        augmentation_transforms[\"reverse time agnostic\"] = RandomReverseTimeVideo(0.5)\n",
    "\n",
    "        return augmentation_transforms\n",
    "\n",
    "    def jitter_fps(self, framerate):\n",
    "        if self.augmentation_types_todo and \"jitter_fps\" in self.augmentation_types_todo:\n",
    "            jitter_factor = np.random.choice(self.fps_jitter_factors)\n",
    "            return int(jitter_factor * framerate)\n",
    "        else:\n",
    "            return framerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def load_args():\n",
    "    parser = argparse.ArgumentParser(description='Smth-Smth example training')\n",
    "    parser.add_argument('--config', '-c', help='json config file path')\n",
    "    parser.add_argument('--eval_only', '-e', action='store_true', \n",
    "                        help=\"evaluate trained model on validation data.\")\n",
    "    parser.add_argument('--resume', '-r', action='store_true',\n",
    "                        help=\"resume training from a given checkpoint.\")\n",
    "    parser.add_argument('--gpus', '-g', help=\"GPU ids to use. Please\"\n",
    "                         \" enter a comma separated list\")\n",
    "    parser.add_argument('--use_cuda', action='store_true',\n",
    "                        help=\"to use GPUs\")\n",
    "    args = parser.parse_args()\n",
    "    if len(sys.argv) < 2:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "    return args\n",
    "\n",
    "\n",
    "def remove_module_from_checkpoint_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Removes the prefix `module` from weight names that gets added by\n",
    "    torch.nn.DataParallel()\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]  # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def load_json_config(path):\n",
    "    \"\"\" loads a json config file\"\"\"\n",
    "    with open(path) as data_file:\n",
    "        config = json.load(data_file)\n",
    "        config = config_init(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "def config_init(config):\n",
    "    \"\"\" Some of the variables that should exist and contain default values \"\"\"\n",
    "    if \"augmentation_mappings_json\" not in config:\n",
    "        config[\"augmentation_mappings_json\"] = None\n",
    "    if \"augmentation_types_todo\" not in config:\n",
    "        config[\"augmentation_types_todo\"] = None\n",
    "    return config\n",
    "\n",
    "\n",
    "def setup_cuda_devices(args):\n",
    "    device_ids = []\n",
    "    device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    if device.type == \"cuda\":\n",
    "        device_ids = [int(i) for i in args.gpus.split(',')]\n",
    "    return device, device_ids\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, config, filename='checkpoint.pth.tar'):\n",
    "    checkpoint_path = os.path.join(config['output_dir'], config['model_name'], filename)\n",
    "    model_path = os.path.join(config['output_dir'], config['model_name'], 'model_best.pth.tar')\n",
    "    torch.save(state, checkpoint_path)\n",
    "    if is_best:\n",
    "        print(\" > Best model found at this epoch. Saving ...\")\n",
    "        shutil.copyfile(checkpoint_path, model_path)\n",
    "\n",
    "\n",
    "def save_results(logits_matrix, features_matrix, targets_list, item_id_list,\n",
    "                 class_to_idx, config):\n",
    "    \"\"\"\n",
    "    Saves the predicted logits matrix, true labels, sample ids and class\n",
    "    dictionary for further analysis of results\n",
    "    \"\"\"\n",
    "    print(\"Saving inference results ...\")\n",
    "    path_to_save = os.path.join(\n",
    "        config['output_dir'], config['model_name'], \"test_results.pkl\")\n",
    "    with open(path_to_save, \"wb\") as f:\n",
    "        pickle.dump([logits_matrix, features_matrix, targets_list,\n",
    "                     item_id_list, class_to_idx], f)\n",
    "\n",
    "\n",
    "def save_images_for_debug(dir_img, imgs):\n",
    "    \"\"\"\n",
    "    2x3x12x224x224 --> [BS, C, seq_len, H, W]\n",
    "    \"\"\"\n",
    "    print(\"Saving images to {}\".format(dir_img))\n",
    "    from matplotlib import pylab as plt\n",
    "    imgs = imgs.permute(0, 2, 3, 4, 1)  # [BS, seq_len, H, W, C]\n",
    "    imgs = imgs.mul(255).numpy()\n",
    "    if not os.path.exists(dir_img):\n",
    "        os.makedirs(dir_img)\n",
    "    print(imgs.shape)\n",
    "    for batch_id, batch in enumerate(imgs):\n",
    "        batch_dir = os.path.join(dir_img, \"batch{}\".format(batch_id + 1))\n",
    "        if not os.path.exists(batch_dir):\n",
    "            os.makedirs(batch_dir)\n",
    "        for j, img in enumerate(batch):\n",
    "            plt.imsave(os.path.join(batch_dir, \"frame{%04d}.png\" % (j + 1)),\n",
    "                       img.astype(\"uint8\"))\n",
    "\n",
    "\n",
    "def get_submission(logits_matrix, item_id_list, class_to_idx, config):\n",
    "    top5_classes_pred_list = []\n",
    "\n",
    "    for i, id in enumerate(item_id_list):\n",
    "        logits_sample = logits_matrix[i]\n",
    "        logits_sample_top5  = logits_sample.argsort()[-5:][::-1]\n",
    "        # top1_class_index = logits_sample.argmax()\n",
    "        # top1_class_label = class_to_idx[top1_class_index]\n",
    "\n",
    "        top5_classes_pred_list.append(logits_sample_top5)\n",
    "\n",
    "    path_to_save = os.path.join(\n",
    "            config['output_dir'], config['model_name'], \"test_submission.csv\")\n",
    "    with open(path_to_save, 'w') as fw:\n",
    "        for id, top5_pred in zip(item_id_list, top5_classes_pred_list):\n",
    "            fw.write(\"{}\".format(id))\n",
    "            for elem in top5_pred:\n",
    "                fw.write(\";{}\".format(elem))\n",
    "            fw.write(\"\\n\")\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class ExperimentalRunCleaner(object):\n",
    "    \"\"\"\n",
    "    Remove the output dir, if you exit with Ctrl+C and if there are less\n",
    "    then 1 file. It prevents the noise of experimental runs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def __call__(self, signal, frame):\n",
    "        num_files = len(glob.glob(self.save_dir + \"/*\"))\n",
    "        if num_files < 1:\n",
    "            print('Removing: {}'.format(self.save_dir))\n",
    "            shutil.rmtree(self.save_dir)\n",
    "        print('You pressed Ctrl+C!')\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video parser\n",
    "\n",
    "# PyAV is for direct and precise access to your media via containers, streams, packets, codecs, and frames.\n",
    "# It exposes a few transformations of that data, and helps you get your data to/from other packages (e.g. Numpy and Pillow).\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFolder(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, json_file_input, json_file_labels, clip_size,\n",
    "                nclips, step_size, is_val, transform_pre=None, transform_post=None,\n",
    "                augmentation_mappings_json=None, augmentation_types_todo=None,\n",
    "                get_item_id=False, is_test=False):\n",
    "        self.dataset_object = WebMDataset(json_file_input, json_file_labels,\n",
    "                                          root, is_test=is_test)\n",
    "        self.json_data = self.dataset_object.json_data\n",
    "        self.classes = self.dataset_object.classes\n",
    "        self.classes_dict = self.dataset_object.classes_dict\n",
    "        self.root = root\n",
    "        self.transform_pre = transform_pre\n",
    "        self.transform_post = transform_post\n",
    "\n",
    "        self.augmentor = Augmentor(augmentation_mappings_json,\n",
    "                                   augmentation_types_todo)\n",
    "\n",
    "        self.clip_size = clip_size\n",
    "        self.nclips = nclips\n",
    "        self.step_size = step_size\n",
    "        self.is_val = is_val\n",
    "        self.get_item_id = get_item_id\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        [!] FPS jittering doesn't work with AV dataloader as of now\n",
    "        \"\"\"\n",
    "\n",
    "        item = self.json_data[index]\n",
    "\n",
    "        # Open video file\n",
    "        reader = av.open(item.path)\n",
    "\n",
    "        try:\n",
    "            imgs = []\n",
    "            imgs = [f.to_rgb().to_nd_array() for f in reader.decode(video=0)]\n",
    "        except (RuntimeError, ZeroDivisionError) as exception:\n",
    "            print('{}: WEBM reader cannot open {}. Empty '\n",
    "                  'list returned.'.format(type(exception).__name__, item.path))\n",
    "\n",
    "        imgs = self.transform_pre(imgs)\n",
    "        imgs, label = self.augmentor(imgs, item.label)\n",
    "        imgs = self.transform_post(imgs)\n",
    "\n",
    "        num_frames = len(imgs)\n",
    "        target_idx = self.classes_dict[label]\n",
    "\n",
    "        if self.nclips > -1:\n",
    "            num_frames_necessary = self.clip_size * self.nclips * self.step_size\n",
    "        else:\n",
    "            num_frames_necessary = num_frames\n",
    "        offset = 0\n",
    "        if num_frames_necessary < num_frames:\n",
    "            # If there are more frames, then sample starting offset.\n",
    "            diff = (num_frames - num_frames_necessary)\n",
    "            # temporal augmentation\n",
    "            if not self.is_val:\n",
    "                offset = np.random.randint(0, diff)\n",
    "\n",
    "        imgs = imgs[offset: num_frames_necessary + offset: self.step_size]\n",
    "\n",
    "        if len(imgs) < (self.clip_size * self.nclips):\n",
    "            imgs.extend([imgs[-1]] *\n",
    "                        ((self.clip_size * self.nclips) - len(imgs)))\n",
    "\n",
    "        # format data to torch\n",
    "        data = torch.stack(imgs)\n",
    "        data = data.permute(1, 0, 2, 3)\n",
    "        if self.get_item_id:\n",
    "            return (data, target_idx, item.id)\n",
    "        else:\n",
    "            return (data, target_idx)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/16892 [00:00<?, ?it/s]C:\\Users\\Light\\anaconda3\\envs\\capstone-project\\lib\\site-packages\\ipykernel_launcher.py:37: AttributeRenamedWarning: VideoFrame.to_nd_array is deprecated; please use VideoFrame.to_ndarray.\n",
      "  1%|â–                                                                | 101/16892 [01:56<5:22:47,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size --> torch.Size([10, 3, 36, 84, 84])\n",
      "116.50028204917908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing VideoFolder class\n",
    "\n",
    "upscale_size = int(84 * 1.1)\n",
    "transform_pre = ComposeMix([\n",
    "                    [Scale(upscale_size), \"img\"],\n",
    "                    [RandomCropVideo(84), \"vid\"]\n",
    "                ])\n",
    "\n",
    "transform_post = ComposeMix([\n",
    "                        [torchvision.transforms.ToTensor(), \"img\"]\n",
    "                    ])\n",
    "\n",
    "loader = VideoFolder(root = \"D:/something-something-project/data/videos/20bn-something-something-v2/\",\n",
    "                    json_file_input = \"D:/something-something-project/data/something-something-v2-train.json\",\n",
    "                    json_file_labels = \"D:/something-something-project/data/something-something-v2-labels.json\",\n",
    "                    clip_size = 36,\n",
    "                    nclips = 1,\n",
    "                    step_size = 1,\n",
    "                    is_val = False,\n",
    "                    transform_pre = transform_pre,\n",
    "                    transform_post = transform_post)\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# change the number of workers to 8 or something since jupyter notebook has some issues. Using 0 works fine\n",
    "batch_loader = torch.utils.data.DataLoader(loader, batch_size=10, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, a in enumerate(tqdm(batch_loader)):\n",
    "    if i > 100:\n",
    "        break\n",
    "    pass\n",
    "\n",
    "print (\"Size --> {}\".format(a[0].size()))\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pylab as plt\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING CALLBACKS\n",
    "###############################################################################\n",
    "\n",
    "class PlotLearning(object):\n",
    "    def __init__(self, save_path, num_classes):\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.save_path_loss = os.path.join(save_path, 'loss_plot.png')\n",
    "        self.save_path_accu = os.path.join(save_path, 'accu_plot.png')\n",
    "        self.save_path_lr = os.path.join(save_path, 'lr_plot.png')\n",
    "        self.init_loss = -np.log(1.0 / num_classes)\n",
    "\n",
    "    def plot(self, logs):\n",
    "        self.accuracy.append(logs.get('acc'))\n",
    "        self.val_accuracy.append(logs.get('val_acc'))\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.learning_rates.append(logs.get('learning_rate'))\n",
    "\n",
    "        best_val_acc = max(self.val_accuracy)\n",
    "        best_train_acc = max(self.accuracy)\n",
    "        best_val_epoch = self.val_accuracy.index(best_val_acc)\n",
    "        best_train_epoch = self.accuracy.index(best_train_acc)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.gca().cla()\n",
    "        plt.ylim(0, 1)\n",
    "        plt.plot(self.accuracy, label='train')\n",
    "        plt.plot(self.val_accuracy, label='valid')\n",
    "        plt.title(\"best_val@{0:}-{1:.2f}, best_train@{2:}-{3:.2f}\".format(\n",
    "            best_val_epoch, best_val_acc, best_train_epoch, best_train_acc))\n",
    "        plt.legend()\n",
    "        plt.savefig(self.save_path_accu)\n",
    "\n",
    "        best_val_loss = min(self.val_losses)\n",
    "        best_train_loss = min(self.losses)\n",
    "        best_val_epoch = self.val_losses.index(best_val_loss)\n",
    "        best_train_epoch = self.losses.index(best_train_loss)\n",
    "\n",
    "        plt.figure(2)\n",
    "        plt.gca().cla()\n",
    "        plt.ylim(0, self.init_loss)\n",
    "        plt.plot(self.losses, label='train')\n",
    "        plt.plot(self.val_losses, label='valid')\n",
    "        plt.title(\"best_val@{0:}-{1:.2f}, best_train@{2:}-{3:.2f}\".format(\n",
    "            best_val_epoch, best_val_loss, best_train_epoch, best_train_loss))\n",
    "        plt.legend()\n",
    "        plt.savefig(self.save_path_loss)\n",
    "\n",
    "        min_learning_rate = min(self.learning_rates)\n",
    "        max_learning_rate = max(self.learning_rates)\n",
    "\n",
    "        plt.figure(2)\n",
    "        plt.gca().cla()\n",
    "        plt.ylim(0, max_learning_rate)\n",
    "        plt.plot(self.learning_rates)\n",
    "        plt.title(\"max_learning_rate-{0:.6f}, min_learning_rate-{1:.6f}\".format(max_learning_rate, min_learning_rate))\n",
    "        plt.savefig(self.save_path_lr)\n",
    "\n",
    "\n",
    "# Taken from keras.keras.utils.generic_utils\n",
    "class Progbar(object):\n",
    "    \"\"\"Displays a progress bar.\n",
    "    # Arguments\n",
    "        target: Total number of steps expected.\n",
    "        interval: Minimum visual progress update interval (in seconds).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, width=30, verbose=1, interval=0.05):\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.sum_values = {}\n",
    "        self.unique_values = []\n",
    "        self.start = time.time()\n",
    "        self.last_update = 0\n",
    "        self.interval = interval\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def update(self, current, values=None, force=False):\n",
    "        \"\"\"Updates the progress bar.\n",
    "        # Arguments\n",
    "            current: Index of current step.\n",
    "            values: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display averages for these values.\n",
    "            force: Whether to force visual progress update.\n",
    "        \"\"\"\n",
    "        values = values or []\n",
    "        for k, v in values:\n",
    "            if k not in self.sum_values:\n",
    "                self.sum_values[k] = [v * (current - self.seen_so_far),\n",
    "                                      current - self.seen_so_far]\n",
    "                self.unique_values.append(k)\n",
    "            else:\n",
    "                self.sum_values[k][0] += v * (current - self.seen_so_far)\n",
    "                self.sum_values[k][1] += (current - self.seen_so_far)\n",
    "        self.seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        if self.verbose == 1:\n",
    "            if not force and (now - self.last_update) < self.interval:\n",
    "                return\n",
    "\n",
    "            prev_total_width = self.total_width\n",
    "            sys.stdout.write('\\b' * prev_total_width)\n",
    "            sys.stdout.write('\\r')\n",
    "\n",
    "            numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n",
    "            bar = barstr % (current, self.target)\n",
    "            prog = float(current) / self.target\n",
    "            prog_width = int(self.width * prog)\n",
    "            if prog_width > 0:\n",
    "                bar += ('=' * (prog_width - 1))\n",
    "                if current < self.target:\n",
    "                    bar += '>'\n",
    "                else:\n",
    "                    bar += '='\n",
    "            bar += ('.' * (self.width - prog_width))\n",
    "            bar += ']'\n",
    "            sys.stdout.write(bar)\n",
    "            self.total_width = len(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self.start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            eta = time_per_unit * (self.target - current)\n",
    "            info = ''\n",
    "            if current < self.target:\n",
    "                info += ' - ETA: %ds' % eta\n",
    "            else:\n",
    "                info += ' - %ds' % (now - self.start)\n",
    "            for k in self.unique_values:\n",
    "                info += ' - %s:' % k\n",
    "                if isinstance(self.sum_values[k], list):\n",
    "                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n",
    "                    if abs(avg) > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                else:\n",
    "                    info += ' %s' % self.sum_values[k]\n",
    "\n",
    "            self.total_width += len(info)\n",
    "            if prev_total_width > self.total_width:\n",
    "                info += ((prev_total_width - self.total_width) * ' ')\n",
    "\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if current >= self.target:\n",
    "                sys.stdout.write('\\n')\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            if current >= self.target:\n",
    "                info = '%ds' % (now - self.start)\n",
    "                for k in self.unique_values:\n",
    "                    info += ' - %s:' % k\n",
    "                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n",
    "                    if avg > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                sys.stdout.write(info + \"\\n\")\n",
    "\n",
    "        self.last_update = now\n",
    "\n",
    "    def add(self, n, values=None):\n",
    "        self.update(self.seen_so_far + n, values)\n",
    "\n",
    "\n",
    "# Taken from PyTorch's examples.imagenet.main\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig a model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    - A 3D CNN with 11 layers.\n",
    "    - Kernel size is kept 3 for all three dimensions - (time, H, W)\n",
    "      except the first layer has kernel size of (3, 5, 5)\n",
    "    - Time dimension is preserved with `padding=1` and `stride=1`, and is\n",
    "      averaged at the end\n",
    "    Arguments:\n",
    "    - Input: a (batch_size, 3, sequence_length, W, H) tensor\n",
    "    - Returns: a (batch_size, 512) sized tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column_units):\n",
    "        super(Model, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=(3, 5, 5), stride=(1, 2, 2), dilation=(1, 1, 1), padding=(1, 2, 2)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p=0.2),\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=1, dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 2, 2), dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p=0.2),\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv3d(128, 128, kernel_size=(3, 3, 3), stride=1, dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=(3, 3, 3), stride=1, dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2), dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p=0.2),\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), stride=1, dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), stride=1, dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p=0.2),\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), stride=1, dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), dilation=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get convolution column features\n",
    "\n",
    "        x = self.block1(x)\n",
    "        # print(x.size())\n",
    "        x = self.block2(x)\n",
    "        # print(x.size())\n",
    "        x = self.block3(x)\n",
    "        # print(x.size())\n",
    "        x = self.block4(x)\n",
    "        # print(x.size())\n",
    "        x = self.block5(x)\n",
    "        # print(x.size())\n",
    "\n",
    "        # averaging features in time dimension\n",
    "        x = x.mean(-1).mean(-1).mean(-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512])\n"
     ]
    }
   ],
   "source": [
    "# testing model\n",
    "\n",
    "num_classes = 174\n",
    "input_tensor = torch.autograd.Variable(torch.rand(5, 3, 72, 84, 84))\n",
    "model = Model(512).cuda()\n",
    "\n",
    "output = model(input_tensor.cuda())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multicolumn\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "\n",
    "class MultiColumn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, conv_column, column_units,\n",
    "                 clf_layers=None):\n",
    "        \"\"\"\n",
    "        - Example multi-column network\n",
    "        - Useful when a video sample is too long and has to be split into\n",
    "          multiple clips\n",
    "        - Processes 3D-CNN on each clip and averages resulting features across\n",
    "          clips before passing it to classification(FC) layer\n",
    "        Args:\n",
    "        - Input: Takes in a list of tensors each of size\n",
    "                 (batch_size, 3, sequence_length, W, H)\n",
    "        - Returns: logits of size (batch size, num_classes)\n",
    "        \"\"\"\n",
    "        super(MultiColumn, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.column_units = column_units\n",
    "        self.conv_column = conv_column(column_units)\n",
    "        self.clf_layers = clf_layers\n",
    "\n",
    "        if not self.clf_layers:\n",
    "            self.clf_layers = th.nn.Sequential(\n",
    "                                 nn.Linear(column_units, self.num_classes)\n",
    "                                )\n",
    "\n",
    "    def forward(self, inputs, get_features=False):\n",
    "        outputs = []\n",
    "        num_cols = len(inputs)\n",
    "        for idx in range(num_cols):\n",
    "            x = inputs[idx]\n",
    "            x1 = self.conv_column(x)\n",
    "            outputs.append(x1)\n",
    "\n",
    "        outputs = th.stack(outputs).permute(1, 0, 2)\n",
    "        outputs = th.squeeze(th.sum(outputs, 1), 1)\n",
    "        avg_output = outputs / float(num_cols)\n",
    "        outputs = self.clf_layers(avg_output)\n",
    "        if get_features:\n",
    "            return outputs, avg_output\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 174])\n"
     ]
    }
   ],
   "source": [
    "num_classes = 174\n",
    "input_tensor = [th.autograd.Variable(th.rand(1, 3, 72, 84, 84))]\n",
    "model = MultiColumn(174, Model, 512)\n",
    "output = model(input_tensor)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import signal\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictX(dict):\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError as k:\n",
    "            raise AttributeError(k)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "    def __delattr__(self, key):\n",
    "        try:\n",
    "            del self[key]\n",
    "        except KeyError as k:\n",
    "            raise AttributeError(k)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<DictX ' + dict.__repr__(self) + '>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DictX ({\n",
    "    \"use_cuda\": True,\n",
    "    \"gpus\": \"0\",\n",
    "    \"resume\": True,\n",
    "    \"eval_only\": False,\n",
    "    \"start_epoch\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using device: cuda\n",
      " > Active GPU ids: [0]\n"
     ]
    }
   ],
   "source": [
    "config = config # defined in the first cell\n",
    "file_name = config['conv_model']\n",
    "# cnn_def = importlib.import_module(\"{}\".format(file_name))\n",
    "\n",
    "\n",
    "# def setup_cuda_devices(args):\n",
    "#     device_ids = []\n",
    "#     device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "#     if device.type == \"cuda\":\n",
    "#         device_ids = [int(i) for i in args.gpus.split(',')]\n",
    "#     return device, device_ids\n",
    "\n",
    "\n",
    "# setup cuda device - CPU or GPU\n",
    "device, device_ids = setup_cuda_devices(args)\n",
    "\n",
    "print(\" > Using device: {}\".format(device.type))\n",
    "print(\" > Active GPU ids: {}\".format(device_ids))\n",
    "\n",
    "best_loss = float('Inf')\n",
    "\n",
    "# dont need this as of now\n",
    "\n",
    "# if config[\"input_mode\"] == \"av\":\n",
    "#     from data_loader_av import VideoFolder\n",
    "# elif config[\"input_mode\"] == \"skvideo\":\n",
    "#     from data_loader_skvideo import VideoFolder\n",
    "# else:\n",
    "#     raise ValueError(\"Please provide a valid input mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, best_loss\n",
    "\n",
    "    # set run output folder\n",
    "    model_name = config[\"model_name\"]\n",
    "    output_dir = config[\"output_dir\"]\n",
    "    save_dir = os.path.join(output_dir, model_name)\n",
    "    print(\" > Output folder for this run -- {}\".format(save_dir))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        os.makedirs(os.path.join(save_dir, 'plots'))\n",
    "\n",
    "    # assign Ctrl+C signal handler\n",
    "    signal.signal(signal.SIGINT, ExperimentalRunCleaner(save_dir))\n",
    "\n",
    "    # create model\n",
    "    print(\" > Creating model ... !\")\n",
    "    model = MultiColumn(config['num_classes'], Model,\n",
    "                        int(config[\"column_units\"]))\n",
    "\n",
    "    # multi GPU setting\n",
    "    model = torch.nn.DataParallel(model, device_ids).to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    checkpoint_path = os.path.join(config['output_dir'],\n",
    "                                   config['model_name'],\n",
    "                                   'model_best.pth.tar')\n",
    "    if args.resume:\n",
    "        if os.path.isfile(checkpoint_path):\n",
    "            print(\" > Loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\" > Loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(checkpoint_path, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\" !#! No checkpoint found at '{}'\".format(\n",
    "                checkpoint_path))\n",
    "\n",
    "    # define augmentation pipeline\n",
    "    upscale_size_train = int(config['input_spatial_size'] * config[\"upscale_factor_train\"])\n",
    "    upscale_size_eval = int(config['input_spatial_size'] * config[\"upscale_factor_eval\"])\n",
    "\n",
    "    # Random crop videos during training\n",
    "    transform_train_pre = ComposeMix([\n",
    "            [RandomRotationVideo(15), \"vid\"],\n",
    "            [Scale(upscale_size_train), \"img\"],\n",
    "            [RandomCropVideo(config['input_spatial_size']), \"vid\"],\n",
    "             ])\n",
    "\n",
    "    # Center crop videos during evaluation\n",
    "    transform_eval_pre = ComposeMix([\n",
    "            [Scale(upscale_size_eval), \"img\"],\n",
    "            [torchvision.transforms.ToPILImage(), \"img\"],\n",
    "            [torchvision.transforms.CenterCrop(config['input_spatial_size']), \"img\"],\n",
    "             ])\n",
    "\n",
    "    # Transforms common to train and eval sets and applied after \"pre\" transforms\n",
    "    transform_post = ComposeMix([\n",
    "            [torchvision.transforms.ToTensor(), \"img\"],\n",
    "            [torchvision.transforms.Normalize(\n",
    "                       mean=[0.485, 0.456, 0.406],  # default values for imagenet\n",
    "                       std=[0.229, 0.224, 0.225]), \"img\"]\n",
    "             ])\n",
    "\n",
    "    train_data = VideoFolder(root=config['data_folder'],\n",
    "                             json_file_input=config['json_data_train'],\n",
    "                             json_file_labels=config['json_data_labels'],\n",
    "                             clip_size=config['clip_size'],\n",
    "                             nclips=config['nclips_train'],\n",
    "                             step_size=config['step_size_train'],\n",
    "                             is_val=False,\n",
    "                             transform_pre=transform_train_pre,\n",
    "                             transform_post=transform_post,\n",
    "                             augmentation_mappings_json=None,\n",
    "                             augmentation_types_todo=None,\n",
    "                             get_item_id=False,\n",
    "                             )\n",
    "\n",
    "    print(\" > Using {} processes for data loader.\".format(\n",
    "        config[\"num_workers\"]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'], shuffle=True,\n",
    "        num_workers=config['num_workers'], pin_memory=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_data = VideoFolder(root=config['data_folder'],\n",
    "                           json_file_input=config['json_data_val'],\n",
    "                           json_file_labels=config['json_data_labels'],\n",
    "                           clip_size=config['clip_size'],\n",
    "                           nclips=config['nclips_val'],\n",
    "                           step_size=config['step_size_val'],\n",
    "                           is_val=True,\n",
    "                           transform_pre=transform_eval_pre,\n",
    "                           transform_post=transform_post,\n",
    "                           get_item_id=True,\n",
    "                           )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_data,\n",
    "        batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "    test_data = VideoFolder(root=config['data_folder'],\n",
    "                            json_file_input=config['json_data_test'],\n",
    "                            json_file_labels=config['json_data_labels'],\n",
    "                            clip_size=config['clip_size'],\n",
    "                            nclips=config['nclips_val'],\n",
    "                            step_size=config['step_size_val'],\n",
    "                            is_val=True,\n",
    "                            transform_pre=transform_eval_pre,\n",
    "                            transform_post=transform_post,\n",
    "                            get_item_id=True,\n",
    "                            is_test=True,\n",
    "                            )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=config['batch_size'], shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "    print(\" > Number of dataset classes : {}\".format(len(train_data.classes)))\n",
    "    assert len(train_data.classes) == config[\"num_classes\"]\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # define optimizer\n",
    "    lr = config[\"lr\"]\n",
    "    last_lr = config[\"last_lr\"]\n",
    "    momentum = config['momentum']\n",
    "    weight_decay = config['weight_decay']\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "    if args.eval_only:\n",
    "        validate(val_loader, model, criterion, train_data.classes_dict)\n",
    "        print(\" > Evaluation DONE !\")\n",
    "        return\n",
    "\n",
    "    # set callbacks\n",
    "    plotter = PlotLearning(os.path.join(\n",
    "        save_dir, \"plots\"), config[\"num_classes\"])\n",
    "    lr_decayer = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                        optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "    val_loss = float('Inf')\n",
    "\n",
    "    # set end condition by num epochs\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    if num_epochs == -1:\n",
    "        num_epochs = 999999\n",
    "\n",
    "    print(\" > Training is getting started...\")\n",
    "    print(\" > Training takes {} epochs.\".format(num_epochs))\n",
    "    start_epoch = args.start_epoch if args.resume else 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        lrs = [params['lr'] for params in optimizer.param_groups]\n",
    "        print(\" > Current LR(s) -- {}\".format(lrs))\n",
    "        if np.max(lr) < last_lr and last_lr > 0:\n",
    "            print(\" > Training is DONE by learning rate {}\".format(last_lr))\n",
    "            sys.exit(1)\n",
    "\n",
    "        # train for one epoch\n",
    "        train_loss, train_top1, train_top5 = train(\n",
    "            train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss, val_top1, val_top5 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # set learning rate\n",
    "        lr_decayer.step(val_loss, epoch)\n",
    "\n",
    "        # plot learning\n",
    "        plotter_dict = {}\n",
    "        plotter_dict['loss'] = train_loss\n",
    "        plotter_dict['val_loss'] = val_loss\n",
    "        plotter_dict['acc'] = train_top1 / 100\n",
    "        plotter_dict['val_acc'] = val_top1 / 100\n",
    "        plotter_dict['learning_rate'] = lr\n",
    "        plotter.plot(plotter_dict)\n",
    "\n",
    "        print(\" > Validation loss after epoch {} = {}\".format(epoch, val_loss))\n",
    "\n",
    "        # remember best loss and save the checkpoint\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': \"Conv4Col\",\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "        }, is_best, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if config['nclips_train'] > 1:\n",
    "            input_var = list(input.split(config['clip_size'], 2))\n",
    "            for idx, inp in enumerate(input_var):\n",
    "                input_var[idx] = inp.to(device)\n",
    "        else:\n",
    "            input_var = [input.to(device)]\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.detach().cpu(), target.detach().cpu(), topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % config[\"print_freq\"] == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, class_to_idx=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    logits_matrix = []\n",
    "    features_matrix = []\n",
    "    targets_list = []\n",
    "    item_id_list = []\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target, item_id) in enumerate(val_loader):\n",
    "\n",
    "            if config['nclips_val'] > 1:\n",
    "                input_var = list(input.split(config['clip_size'], 2))\n",
    "                for idx, inp in enumerate(input_var):\n",
    "                    input_var[idx] = inp.to(device)\n",
    "            else:\n",
    "                input_var = [input.to(device)]\n",
    "\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output and loss\n",
    "            output, features = model(input_var, config['save_features'])\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            if args.eval_only:\n",
    "                logits_matrix.append(output.cpu().data.numpy())\n",
    "                features_matrix.append(features.cpu().data.numpy())\n",
    "                targets_list.append(target.cpu().numpy())\n",
    "                item_id_list.append(item_id)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output.detach().cpu(), target.detach().cpu(), topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "            top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % config[\"print_freq\"] == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                          i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    if args.eval_only:\n",
    "        logits_matrix = np.concatenate(logits_matrix)\n",
    "        features_matrix = np.concatenate(features_matrix)\n",
    "        targets_list = np.concatenate(targets_list)\n",
    "        item_id_list = np.concatenate(item_id_list)\n",
    "        print(logits_matrix.shape, targets_list.shape, item_id_list.shape)\n",
    "        save_results(logits_matrix, features_matrix, targets_list,\n",
    "                     item_id_list, class_to_idx, config)\n",
    "        get_submission(logits_matrix, item_id_list, class_to_idx, config)\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Output folder for this run -- trained_models\\3D_model\n",
      " > Creating model ... !\n",
      " !#! No checkpoint found at 'trained_models\\3D_model\\model_best.pth.tar'\n",
      " > Using 0 processes for data loader.\n",
      " > Number of dataset classes : 174\n",
      " > Training is getting started...\n",
      " > Training takes 30 epochs.\n",
      " > Current LR(s) -- [0.008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Light\\anaconda3\\envs\\capstone-project\\lib\\site-packages\\ipykernel_launcher.py:37: AttributeRenamedWarning: VideoFrame.to_nd_array is deprecated; please use VideoFrame.to_ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/5630]\tTime 6.183 (6.183)\tData 4.504 (4.504)\tLoss 5.2203 (5.2203)\tPrec@1 0.000 (0.000)\tPrec@5 3.333 (3.333)\n",
      "Epoch: [0][1/5630]\tTime 4.809 (5.496)\tData 4.037 (4.271)\tLoss 5.2314 (5.2259)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (1.667)\n",
      "Epoch: [0][2/5630]\tTime 5.023 (5.339)\tData 4.253 (4.265)\tLoss 5.1179 (5.1899)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (1.111)\n",
      "Epoch: [0][3/5630]\tTime 4.988 (5.251)\tData 4.219 (4.253)\tLoss 5.1124 (5.1705)\tPrec@1 0.000 (0.000)\tPrec@5 6.667 (2.500)\n",
      "Epoch: [0][4/5630]\tTime 4.728 (5.146)\tData 3.952 (4.193)\tLoss 5.1465 (5.1657)\tPrec@1 6.667 (1.333)\tPrec@5 10.000 (4.000)\n",
      "Epoch: [0][5/5630]\tTime 4.978 (5.118)\tData 4.204 (4.195)\tLoss 5.1470 (5.1626)\tPrec@1 0.000 (1.111)\tPrec@5 3.333 (3.889)\n",
      "Epoch: [0][6/5630]\tTime 4.960 (5.096)\tData 4.179 (4.193)\tLoss 5.1592 (5.1621)\tPrec@1 3.333 (1.429)\tPrec@5 10.000 (4.762)\n",
      "Epoch: [0][7/5630]\tTime 4.791 (5.058)\tData 4.005 (4.169)\tLoss 5.0685 (5.1504)\tPrec@1 0.000 (1.250)\tPrec@5 10.000 (5.417)\n",
      "Epoch: [0][8/5630]\tTime 4.789 (5.028)\tData 3.999 (4.150)\tLoss 5.1985 (5.1558)\tPrec@1 0.000 (1.111)\tPrec@5 6.667 (5.556)\n",
      "Epoch: [0][9/5630]\tTime 5.117 (5.037)\tData 4.335 (4.169)\tLoss 5.1814 (5.1583)\tPrec@1 0.000 (1.000)\tPrec@5 3.333 (5.333)\n",
      "Epoch: [0][10/5630]\tTime 4.995 (5.033)\tData 4.215 (4.173)\tLoss 5.1855 (5.1608)\tPrec@1 0.000 (0.909)\tPrec@5 6.667 (5.455)\n",
      "Epoch: [0][11/5630]\tTime 5.102 (5.039)\tData 4.326 (4.186)\tLoss 5.1201 (5.1574)\tPrec@1 3.333 (1.111)\tPrec@5 6.667 (5.556)\n",
      "Epoch: [0][12/5630]\tTime 4.670 (5.010)\tData 3.880 (4.162)\tLoss 5.1474 (5.1566)\tPrec@1 0.000 (1.026)\tPrec@5 10.000 (5.897)\n",
      "Epoch: [0][13/5630]\tTime 5.106 (5.017)\tData 4.321 (4.173)\tLoss 5.2579 (5.1639)\tPrec@1 0.000 (0.952)\tPrec@5 6.667 (5.952)\n",
      "Epoch: [0][14/5630]\tTime 4.745 (4.999)\tData 3.958 (4.159)\tLoss 5.2167 (5.1674)\tPrec@1 0.000 (0.889)\tPrec@5 6.667 (6.000)\n",
      "Epoch: [0][15/5630]\tTime 4.872 (4.991)\tData 4.085 (4.155)\tLoss 5.1625 (5.1671)\tPrec@1 0.000 (0.833)\tPrec@5 3.333 (5.833)\n",
      "Epoch: [0][16/5630]\tTime 4.868 (4.984)\tData 4.074 (4.150)\tLoss 5.1171 (5.1641)\tPrec@1 0.000 (0.784)\tPrec@5 6.667 (5.882)\n",
      "Epoch: [0][17/5630]\tTime 5.024 (4.986)\tData 4.233 (4.154)\tLoss 5.2422 (5.1685)\tPrec@1 0.000 (0.741)\tPrec@5 3.333 (5.741)\n",
      "Epoch: [0][18/5630]\tTime 4.888 (4.981)\tData 4.092 (4.151)\tLoss 5.1819 (5.1692)\tPrec@1 3.333 (0.877)\tPrec@5 6.667 (5.789)\n",
      "Epoch: [0][19/5630]\tTime 4.921 (4.978)\tData 4.121 (4.150)\tLoss 5.1468 (5.1681)\tPrec@1 3.333 (1.000)\tPrec@5 13.333 (6.167)\n",
      "Epoch: [0][20/5630]\tTime 5.103 (4.984)\tData 4.314 (4.157)\tLoss 5.2408 (5.1715)\tPrec@1 0.000 (0.952)\tPrec@5 0.000 (5.873)\n",
      "Epoch: [0][21/5630]\tTime 4.936 (4.982)\tData 4.144 (4.157)\tLoss 5.2078 (5.1732)\tPrec@1 0.000 (0.909)\tPrec@5 3.333 (5.758)\n",
      "Epoch: [0][22/5630]\tTime 5.388 (4.999)\tData 4.594 (4.176)\tLoss 5.0471 (5.1677)\tPrec@1 3.333 (1.014)\tPrec@5 6.667 (5.797)\n",
      "Epoch: [0][23/5630]\tTime 5.214 (5.008)\tData 4.422 (4.186)\tLoss 5.1306 (5.1661)\tPrec@1 3.333 (1.111)\tPrec@5 10.000 (5.972)\n",
      "Epoch: [0][24/5630]\tTime 4.755 (4.998)\tData 3.964 (4.177)\tLoss 5.1304 (5.1647)\tPrec@1 0.000 (1.067)\tPrec@5 6.667 (6.000)\n",
      "Epoch: [0][25/5630]\tTime 5.088 (5.002)\tData 4.295 (4.182)\tLoss 4.9418 (5.1561)\tPrec@1 0.000 (1.026)\tPrec@5 10.000 (6.154)\n",
      "Epoch: [0][26/5630]\tTime 5.024 (5.002)\tData 4.231 (4.184)\tLoss 5.2083 (5.1581)\tPrec@1 0.000 (0.988)\tPrec@5 0.000 (5.926)\n",
      "Epoch: [0][27/5630]\tTime 4.923 (5.000)\tData 4.130 (4.182)\tLoss 5.2422 (5.1611)\tPrec@1 3.333 (1.071)\tPrec@5 3.333 (5.833)\n",
      "Epoch: [0][28/5630]\tTime 6.096 (5.037)\tData 5.302 (4.220)\tLoss 5.0319 (5.1566)\tPrec@1 3.333 (1.149)\tPrec@5 6.667 (5.862)\n",
      "Epoch: [0][29/5630]\tTime 5.168 (5.042)\tData 4.380 (4.226)\tLoss 5.0639 (5.1535)\tPrec@1 0.000 (1.111)\tPrec@5 3.333 (5.778)\n",
      "Epoch: [0][30/5630]\tTime 5.145 (5.045)\tData 4.351 (4.230)\tLoss 5.0870 (5.1514)\tPrec@1 0.000 (1.075)\tPrec@5 0.000 (5.591)\n",
      "Epoch: [0][31/5630]\tTime 4.660 (5.033)\tData 3.859 (4.218)\tLoss 5.0317 (5.1477)\tPrec@1 0.000 (1.042)\tPrec@5 6.667 (5.625)\n",
      "Epoch: [0][32/5630]\tTime 4.992 (5.032)\tData 4.197 (4.218)\tLoss 5.0881 (5.1458)\tPrec@1 0.000 (1.010)\tPrec@5 3.333 (5.556)\n",
      "Epoch: [0][33/5630]\tTime 5.110 (5.034)\tData 4.315 (4.220)\tLoss 5.2762 (5.1497)\tPrec@1 0.000 (0.980)\tPrec@5 3.333 (5.490)\n",
      "Epoch: [0][34/5630]\tTime 5.190 (5.039)\tData 4.392 (4.225)\tLoss 5.1828 (5.1506)\tPrec@1 3.333 (1.048)\tPrec@5 6.667 (5.524)\n",
      "Epoch: [0][35/5630]\tTime 4.990 (5.037)\tData 4.192 (4.224)\tLoss 5.2047 (5.1521)\tPrec@1 0.000 (1.019)\tPrec@5 6.667 (5.556)\n",
      "Epoch: [0][36/5630]\tTime 5.190 (5.041)\tData 4.394 (4.229)\tLoss 5.0946 (5.1506)\tPrec@1 3.333 (1.081)\tPrec@5 10.000 (5.676)\n",
      "Epoch: [0][37/5630]\tTime 4.975 (5.040)\tData 4.173 (4.227)\tLoss 5.0150 (5.1470)\tPrec@1 0.000 (1.053)\tPrec@5 3.333 (5.614)\n",
      "Epoch: [0][38/5630]\tTime 5.288 (5.046)\tData 4.498 (4.234)\tLoss 5.1521 (5.1471)\tPrec@1 0.000 (1.026)\tPrec@5 0.000 (5.470)\n",
      "Epoch: [0][39/5630]\tTime 5.230 (5.051)\tData 4.432 (4.239)\tLoss 5.1559 (5.1474)\tPrec@1 0.000 (1.000)\tPrec@5 3.333 (5.417)\n",
      "Epoch: [0][40/5630]\tTime 5.380 (5.059)\tData 4.577 (4.248)\tLoss 5.2998 (5.1511)\tPrec@1 0.000 (0.976)\tPrec@5 0.000 (5.285)\n",
      "Epoch: [0][41/5630]\tTime 5.298 (5.064)\tData 4.500 (4.254)\tLoss 5.2691 (5.1539)\tPrec@1 3.333 (1.032)\tPrec@5 6.667 (5.317)\n",
      "Epoch: [0][42/5630]\tTime 4.826 (5.059)\tData 4.026 (4.248)\tLoss 5.1552 (5.1539)\tPrec@1 0.000 (1.008)\tPrec@5 6.667 (5.349)\n",
      "Epoch: [0][43/5630]\tTime 5.168 (5.061)\tData 4.371 (4.251)\tLoss 5.1569 (5.1540)\tPrec@1 0.000 (0.985)\tPrec@5 3.333 (5.303)\n",
      "Epoch: [0][44/5630]\tTime 5.017 (5.060)\tData 4.217 (4.250)\tLoss 5.2128 (5.1553)\tPrec@1 3.333 (1.037)\tPrec@5 10.000 (5.407)\n",
      "Epoch: [0][45/5630]\tTime 5.209 (5.064)\tData 4.410 (4.254)\tLoss 5.1691 (5.1556)\tPrec@1 0.000 (1.014)\tPrec@5 0.000 (5.290)\n",
      "Epoch: [0][46/5630]\tTime 4.837 (5.059)\tData 4.028 (4.249)\tLoss 4.9294 (5.1508)\tPrec@1 0.000 (0.993)\tPrec@5 10.000 (5.390)\n",
      "Epoch: [0][47/5630]\tTime 5.246 (5.063)\tData 4.429 (4.253)\tLoss 4.9931 (5.1475)\tPrec@1 3.333 (1.042)\tPrec@5 6.667 (5.417)\n",
      "Epoch: [0][48/5630]\tTime 5.004 (5.061)\tData 4.194 (4.252)\tLoss 5.0220 (5.1449)\tPrec@1 0.000 (1.020)\tPrec@5 3.333 (5.374)\n",
      "Epoch: [0][49/5630]\tTime 5.157 (5.063)\tData 4.330 (4.253)\tLoss 5.1588 (5.1452)\tPrec@1 0.000 (1.000)\tPrec@5 0.000 (5.267)\n",
      "Epoch: [0][50/5630]\tTime 4.861 (5.059)\tData 4.037 (4.249)\tLoss 5.1375 (5.1451)\tPrec@1 0.000 (0.980)\tPrec@5 3.333 (5.229)\n",
      "Epoch: [0][51/5630]\tTime 4.952 (5.057)\tData 4.145 (4.247)\tLoss 5.0563 (5.1434)\tPrec@1 3.333 (1.026)\tPrec@5 3.333 (5.192)\n",
      "Epoch: [0][52/5630]\tTime 4.945 (5.055)\tData 4.032 (4.243)\tLoss 4.9551 (5.1398)\tPrec@1 0.000 (1.006)\tPrec@5 13.333 (5.346)\n",
      "Epoch: [0][53/5630]\tTime 4.921 (5.053)\tData 4.009 (4.239)\tLoss 5.0879 (5.1388)\tPrec@1 3.333 (1.049)\tPrec@5 6.667 (5.370)\n",
      "Epoch: [0][54/5630]\tTime 5.234 (5.056)\tData 4.377 (4.241)\tLoss 5.1812 (5.1396)\tPrec@1 0.000 (1.030)\tPrec@5 3.333 (5.333)\n",
      "Epoch: [0][55/5630]\tTime 5.163 (5.058)\tData 4.307 (4.242)\tLoss 5.1054 (5.1390)\tPrec@1 0.000 (1.012)\tPrec@5 0.000 (5.238)\n",
      "Epoch: [0][56/5630]\tTime 5.401 (5.064)\tData 4.407 (4.245)\tLoss 5.0439 (5.1373)\tPrec@1 0.000 (0.994)\tPrec@5 10.000 (5.322)\n",
      "Epoch: [0][57/5630]\tTime 5.274 (5.068)\tData 4.283 (4.246)\tLoss 5.1191 (5.1370)\tPrec@1 0.000 (0.977)\tPrec@5 3.333 (5.287)\n",
      "Epoch: [0][58/5630]\tTime 5.371 (5.073)\tData 4.482 (4.250)\tLoss 5.1283 (5.1369)\tPrec@1 0.000 (0.960)\tPrec@5 6.667 (5.311)\n",
      "Epoch: [0][59/5630]\tTime 5.466 (5.079)\tData 4.463 (4.253)\tLoss 4.9945 (5.1345)\tPrec@1 0.000 (0.944)\tPrec@5 13.333 (5.444)\n",
      "Epoch: [0][60/5630]\tTime 5.525 (5.087)\tData 4.528 (4.258)\tLoss 4.9043 (5.1307)\tPrec@1 6.667 (1.038)\tPrec@5 10.000 (5.519)\n",
      "Epoch: [0][61/5630]\tTime 5.424 (5.092)\tData 4.519 (4.262)\tLoss 5.3242 (5.1338)\tPrec@1 0.000 (1.022)\tPrec@5 0.000 (5.430)\n",
      "Epoch: [0][62/5630]\tTime 5.952 (5.106)\tData 4.094 (4.259)\tLoss 5.1523 (5.1341)\tPrec@1 3.333 (1.058)\tPrec@5 6.667 (5.450)\n",
      "Epoch: [0][63/5630]\tTime 6.332 (5.125)\tData 4.164 (4.258)\tLoss 4.9782 (5.1317)\tPrec@1 0.000 (1.042)\tPrec@5 0.000 (5.365)\n",
      "Epoch: [0][64/5630]\tTime 5.491 (5.130)\tData 4.254 (4.258)\tLoss 5.1226 (5.1316)\tPrec@1 0.000 (1.026)\tPrec@5 0.000 (5.282)\n",
      "Epoch: [0][65/5630]\tTime 5.466 (5.136)\tData 4.506 (4.262)\tLoss 5.1292 (5.1315)\tPrec@1 0.000 (1.010)\tPrec@5 3.333 (5.253)\n",
      "Epoch: [0][66/5630]\tTime 5.128 (5.135)\tData 4.174 (4.260)\tLoss 5.1383 (5.1316)\tPrec@1 0.000 (0.995)\tPrec@5 3.333 (5.224)\n",
      "Epoch: [0][67/5630]\tTime 5.303 (5.138)\tData 4.463 (4.263)\tLoss 5.2070 (5.1327)\tPrec@1 0.000 (0.980)\tPrec@5 3.333 (5.196)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][68/5630]\tTime 5.571 (5.144)\tData 4.455 (4.266)\tLoss 5.0213 (5.1311)\tPrec@1 3.333 (1.014)\tPrec@5 10.000 (5.266)\n",
      "Epoch: [0][69/5630]\tTime 5.405 (5.148)\tData 4.154 (4.264)\tLoss 5.0608 (5.1301)\tPrec@1 0.000 (1.000)\tPrec@5 6.667 (5.286)\n",
      "Epoch: [0][70/5630]\tTime 6.029 (5.160)\tData 4.466 (4.267)\tLoss 5.0673 (5.1292)\tPrec@1 0.000 (0.986)\tPrec@5 6.667 (5.305)\n",
      "Epoch: [0][71/5630]\tTime 6.452 (5.178)\tData 4.052 (4.264)\tLoss 5.0424 (5.1280)\tPrec@1 0.000 (0.972)\tPrec@5 3.333 (5.278)\n",
      "Epoch: [0][72/5630]\tTime 6.442 (5.196)\tData 4.243 (4.264)\tLoss 5.0333 (5.1267)\tPrec@1 0.000 (0.959)\tPrec@5 6.667 (5.297)\n",
      "Epoch: [0][73/5630]\tTime 5.657 (5.202)\tData 3.996 (4.260)\tLoss 4.9309 (5.1241)\tPrec@1 3.333 (0.991)\tPrec@5 13.333 (5.405)\n",
      "Epoch: [0][74/5630]\tTime 5.385 (5.204)\tData 4.026 (4.257)\tLoss 5.1987 (5.1251)\tPrec@1 0.000 (0.978)\tPrec@5 3.333 (5.378)\n",
      "Epoch: [0][75/5630]\tTime 5.444 (5.207)\tData 4.322 (4.258)\tLoss 4.9493 (5.1228)\tPrec@1 0.000 (0.965)\tPrec@5 3.333 (5.351)\n",
      "Epoch: [0][76/5630]\tTime 5.510 (5.211)\tData 4.391 (4.260)\tLoss 5.1139 (5.1226)\tPrec@1 0.000 (0.952)\tPrec@5 3.333 (5.325)\n",
      "Epoch: [0][77/5630]\tTime 5.406 (5.214)\tData 4.232 (4.259)\tLoss 5.1442 (5.1229)\tPrec@1 6.667 (1.026)\tPrec@5 6.667 (5.342)\n",
      "Epoch: [0][78/5630]\tTime 5.402 (5.216)\tData 4.111 (4.258)\tLoss 4.9986 (5.1214)\tPrec@1 3.333 (1.055)\tPrec@5 13.333 (5.443)\n",
      "Epoch: [0][79/5630]\tTime 5.408 (5.219)\tData 4.261 (4.258)\tLoss 5.0329 (5.1202)\tPrec@1 0.000 (1.042)\tPrec@5 0.000 (5.375)\n",
      "Epoch: [0][80/5630]\tTime 5.410 (5.221)\tData 3.809 (4.252)\tLoss 5.2093 (5.1213)\tPrec@1 0.000 (1.029)\tPrec@5 3.333 (5.350)\n",
      "Epoch: [0][81/5630]\tTime 5.410 (5.223)\tData 4.189 (4.251)\tLoss 5.1933 (5.1222)\tPrec@1 0.000 (1.016)\tPrec@5 6.667 (5.366)\n",
      "Epoch: [0][82/5630]\tTime 5.417 (5.226)\tData 4.295 (4.252)\tLoss 5.1503 (5.1226)\tPrec@1 0.000 (1.004)\tPrec@5 0.000 (5.301)\n",
      "Epoch: [0][83/5630]\tTime 6.292 (5.238)\tData 4.209 (4.251)\tLoss 5.0947 (5.1222)\tPrec@1 0.000 (0.992)\tPrec@5 3.333 (5.278)\n",
      "Epoch: [0][84/5630]\tTime 6.737 (5.256)\tData 4.224 (4.251)\tLoss 4.9739 (5.1205)\tPrec@1 0.000 (0.980)\tPrec@5 13.333 (5.373)\n",
      "Epoch: [0][85/5630]\tTime 6.736 (5.273)\tData 3.922 (4.247)\tLoss 5.0261 (5.1194)\tPrec@1 0.000 (0.969)\tPrec@5 10.000 (5.426)\n",
      "Epoch: [0][86/5630]\tTime 6.706 (5.290)\tData 4.317 (4.248)\tLoss 5.0591 (5.1187)\tPrec@1 0.000 (0.958)\tPrec@5 6.667 (5.441)\n",
      "Epoch: [0][87/5630]\tTime 6.543 (5.304)\tData 4.321 (4.249)\tLoss 5.1909 (5.1195)\tPrec@1 0.000 (0.947)\tPrec@5 10.000 (5.492)\n",
      "Epoch: [0][88/5630]\tTime 5.564 (5.307)\tData 4.286 (4.249)\tLoss 4.9104 (5.1172)\tPrec@1 0.000 (0.936)\tPrec@5 13.333 (5.581)\n",
      "Epoch: [0][89/5630]\tTime 5.574 (5.310)\tData 4.305 (4.250)\tLoss 5.0429 (5.1163)\tPrec@1 0.000 (0.926)\tPrec@5 6.667 (5.593)\n",
      "Epoch: [0][90/5630]\tTime 5.602 (5.313)\tData 4.282 (4.250)\tLoss 4.9857 (5.1149)\tPrec@1 3.333 (0.952)\tPrec@5 6.667 (5.604)\n",
      "Epoch: [0][91/5630]\tTime 5.610 (5.316)\tData 4.440 (4.252)\tLoss 5.2194 (5.1160)\tPrec@1 0.000 (0.942)\tPrec@5 0.000 (5.543)\n",
      "Epoch: [0][92/5630]\tTime 5.597 (5.319)\tData 3.941 (4.249)\tLoss 4.7868 (5.1125)\tPrec@1 6.667 (1.004)\tPrec@5 16.667 (5.663)\n",
      "Epoch: [0][93/5630]\tTime 6.616 (5.333)\tData 4.589 (4.253)\tLoss 4.8944 (5.1102)\tPrec@1 3.333 (1.028)\tPrec@5 13.333 (5.745)\n",
      "Epoch: [0][94/5630]\tTime 6.890 (5.349)\tData 4.293 (4.253)\tLoss 5.0081 (5.1091)\tPrec@1 3.333 (1.053)\tPrec@5 13.333 (5.825)\n",
      "Epoch: [0][95/5630]\tTime 6.897 (5.365)\tData 4.358 (4.254)\tLoss 5.0958 (5.1090)\tPrec@1 0.000 (1.042)\tPrec@5 3.333 (5.799)\n",
      "Epoch: [0][96/5630]\tTime 6.865 (5.381)\tData 4.107 (4.253)\tLoss 4.9926 (5.1078)\tPrec@1 3.333 (1.065)\tPrec@5 16.667 (5.911)\n",
      "Epoch: [0][97/5630]\tTime 6.832 (5.396)\tData 4.166 (4.252)\tLoss 5.1197 (5.1079)\tPrec@1 6.667 (1.122)\tPrec@5 6.667 (5.918)\n",
      "Epoch: [0][98/5630]\tTime 6.404 (5.406)\tData 4.185 (4.251)\tLoss 4.7956 (5.1047)\tPrec@1 10.000 (1.212)\tPrec@5 16.667 (6.027)\n",
      "Epoch: [0][99/5630]\tTime 5.646 (5.408)\tData 3.918 (4.248)\tLoss 4.7499 (5.1012)\tPrec@1 6.667 (1.267)\tPrec@5 13.333 (6.100)\n",
      "Epoch: [0][100/5630]\tTime 5.700 (5.411)\tData 4.418 (4.249)\tLoss 5.0230 (5.1004)\tPrec@1 0.000 (1.254)\tPrec@5 6.667 (6.106)\n",
      "Epoch: [0][101/5630]\tTime 6.421 (5.421)\tData 4.188 (4.249)\tLoss 5.2281 (5.1017)\tPrec@1 0.000 (1.242)\tPrec@5 3.333 (6.078)\n",
      "Epoch: [0][102/5630]\tTime 6.895 (5.435)\tData 4.091 (4.247)\tLoss 4.9332 (5.1000)\tPrec@1 3.333 (1.262)\tPrec@5 10.000 (6.117)\n",
      "Epoch: [0][103/5630]\tTime 6.882 (5.449)\tData 4.046 (4.245)\tLoss 5.1506 (5.1005)\tPrec@1 0.000 (1.250)\tPrec@5 6.667 (6.122)\n",
      "Epoch: [0][104/5630]\tTime 6.871 (5.463)\tData 4.411 (4.247)\tLoss 4.9352 (5.0989)\tPrec@1 3.333 (1.270)\tPrec@5 6.667 (6.127)\n",
      "Epoch: [0][105/5630]\tTime 6.851 (5.476)\tData 4.023 (4.245)\tLoss 5.0171 (5.0982)\tPrec@1 0.000 (1.258)\tPrec@5 6.667 (6.132)\n",
      "Epoch: [0][106/5630]\tTime 5.476 (5.476)\tData 4.626 (4.248)\tLoss 4.9970 (5.0972)\tPrec@1 6.667 (1.308)\tPrec@5 10.000 (6.168)\n",
      "Epoch: [0][107/5630]\tTime 5.238 (5.474)\tData 4.030 (4.246)\tLoss 5.0678 (5.0969)\tPrec@1 0.000 (1.296)\tPrec@5 3.333 (6.142)\n",
      "Epoch: [0][108/5630]\tTime 5.839 (5.477)\tData 4.212 (4.246)\tLoss 4.8595 (5.0948)\tPrec@1 0.000 (1.284)\tPrec@5 6.667 (6.147)\n",
      "Epoch: [0][109/5630]\tTime 5.844 (5.480)\tData 4.529 (4.249)\tLoss 5.1603 (5.0954)\tPrec@1 0.000 (1.273)\tPrec@5 0.000 (6.091)\n",
      "Epoch: [0][110/5630]\tTime 5.854 (5.484)\tData 4.267 (4.249)\tLoss 5.0461 (5.0949)\tPrec@1 3.333 (1.291)\tPrec@5 10.000 (6.126)\n",
      "Epoch: [0][111/5630]\tTime 5.859 (5.487)\tData 4.150 (4.248)\tLoss 4.8948 (5.0931)\tPrec@1 6.667 (1.339)\tPrec@5 10.000 (6.161)\n",
      "Epoch: [0][112/5630]\tTime 5.829 (5.490)\tData 4.281 (4.248)\tLoss 5.0226 (5.0925)\tPrec@1 0.000 (1.327)\tPrec@5 0.000 (6.106)\n",
      "Epoch: [0][113/5630]\tTime 6.683 (5.501)\tData 4.239 (4.248)\tLoss 5.1193 (5.0927)\tPrec@1 0.000 (1.316)\tPrec@5 3.333 (6.082)\n",
      "Epoch: [0][114/5630]\tTime 7.239 (5.516)\tData 4.261 (4.248)\tLoss 5.0115 (5.0920)\tPrec@1 0.000 (1.304)\tPrec@5 6.667 (6.087)\n",
      "Epoch: [0][115/5630]\tTime 7.222 (5.530)\tData 4.958 (4.254)\tLoss 5.1845 (5.0928)\tPrec@1 3.333 (1.322)\tPrec@5 6.667 (6.092)\n",
      "Epoch: [0][116/5630]\tTime 7.238 (5.545)\tData 4.220 (4.254)\tLoss 4.9659 (5.0918)\tPrec@1 0.000 (1.311)\tPrec@5 6.667 (6.097)\n",
      "Epoch: [0][117/5630]\tTime 7.135 (5.559)\tData 4.335 (4.255)\tLoss 5.3117 (5.0936)\tPrec@1 3.333 (1.328)\tPrec@5 6.667 (6.102)\n",
      "Epoch: [0][118/5630]\tTime 5.871 (5.561)\tData 4.228 (4.254)\tLoss 4.8975 (5.0920)\tPrec@1 3.333 (1.345)\tPrec@5 13.333 (6.162)\n",
      "Epoch: [0][119/5630]\tTime 5.898 (5.564)\tData 4.123 (4.253)\tLoss 4.9184 (5.0905)\tPrec@1 6.667 (1.389)\tPrec@5 6.667 (6.167)\n",
      "Epoch: [0][120/5630]\tTime 5.891 (5.567)\tData 3.983 (4.251)\tLoss 4.9288 (5.0892)\tPrec@1 0.000 (1.377)\tPrec@5 3.333 (6.143)\n",
      "Epoch: [0][121/5630]\tTime 5.907 (5.569)\tData 4.144 (4.250)\tLoss 4.9656 (5.0882)\tPrec@1 3.333 (1.393)\tPrec@5 6.667 (6.148)\n",
      "Epoch: [0][122/5630]\tTime 5.895 (5.572)\tData 4.100 (4.249)\tLoss 4.9987 (5.0874)\tPrec@1 0.000 (1.382)\tPrec@5 3.333 (6.125)\n",
      "Epoch: [0][123/5630]\tTime 5.903 (5.575)\tData 4.214 (4.249)\tLoss 5.0971 (5.0875)\tPrec@1 0.000 (1.371)\tPrec@5 0.000 (6.075)\n",
      "Epoch: [0][124/5630]\tTime 5.899 (5.577)\tData 4.096 (4.247)\tLoss 4.9851 (5.0867)\tPrec@1 0.000 (1.360)\tPrec@5 10.000 (6.107)\n",
      "Epoch: [0][125/5630]\tTime 6.390 (5.584)\tData 3.842 (4.244)\tLoss 5.0452 (5.0864)\tPrec@1 3.333 (1.376)\tPrec@5 6.667 (6.111)\n",
      "Epoch: [0][126/5630]\tTime 7.308 (5.597)\tData 4.069 (4.243)\tLoss 5.0446 (5.0860)\tPrec@1 3.333 (1.391)\tPrec@5 13.333 (6.168)\n",
      "Epoch: [0][127/5630]\tTime 7.307 (5.611)\tData 4.092 (4.242)\tLoss 4.8882 (5.0845)\tPrec@1 3.333 (1.406)\tPrec@5 10.000 (6.198)\n",
      "Epoch: [0][128/5630]\tTime 6.450 (5.617)\tData 4.273 (4.242)\tLoss 4.8893 (5.0830)\tPrec@1 0.000 (1.395)\tPrec@5 10.000 (6.227)\n",
      "Epoch: [0][129/5630]\tTime 5.939 (5.620)\tData 4.352 (4.243)\tLoss 5.0194 (5.0825)\tPrec@1 0.000 (1.385)\tPrec@5 0.000 (6.179)\n",
      "Epoch: [0][130/5630]\tTime 5.955 (5.622)\tData 4.065 (4.241)\tLoss 4.9444 (5.0814)\tPrec@1 0.000 (1.374)\tPrec@5 13.333 (6.234)\n",
      "Epoch: [0][131/5630]\tTime 5.949 (5.625)\tData 4.379 (4.242)\tLoss 4.9208 (5.0802)\tPrec@1 3.333 (1.389)\tPrec@5 10.000 (6.263)\n",
      "Epoch: [0][132/5630]\tTime 6.831 (5.634)\tData 3.971 (4.240)\tLoss 5.2145 (5.0812)\tPrec@1 0.000 (1.378)\tPrec@5 0.000 (6.216)\n",
      "Epoch: [0][133/5630]\tTime 7.501 (5.648)\tData 4.248 (4.241)\tLoss 5.0387 (5.0809)\tPrec@1 3.333 (1.393)\tPrec@5 6.667 (6.219)\n",
      "Epoch: [0][134/5630]\tTime 7.246 (5.660)\tData 4.530 (4.243)\tLoss 5.1124 (5.0812)\tPrec@1 3.333 (1.407)\tPrec@5 3.333 (6.198)\n",
      "Epoch: [0][135/5630]\tTime 6.053 (5.663)\tData 4.102 (4.242)\tLoss 4.9409 (5.0801)\tPrec@1 0.000 (1.397)\tPrec@5 10.000 (6.225)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][136/5630]\tTime 6.054 (5.665)\tData 4.414 (4.243)\tLoss 5.1315 (5.0805)\tPrec@1 0.000 (1.387)\tPrec@5 3.333 (6.204)\n",
      "Epoch: [0][137/5630]\tTime 6.072 (5.668)\tData 4.080 (4.242)\tLoss 5.1464 (5.0810)\tPrec@1 0.000 (1.377)\tPrec@5 6.667 (6.208)\n",
      "Epoch: [0][138/5630]\tTime 6.070 (5.671)\tData 4.527 (4.244)\tLoss 5.0207 (5.0805)\tPrec@1 3.333 (1.391)\tPrec@5 10.000 (6.235)\n",
      "Epoch: [0][139/5630]\tTime 6.063 (5.674)\tData 3.685 (4.240)\tLoss 5.0910 (5.0806)\tPrec@1 0.000 (1.381)\tPrec@5 0.000 (6.190)\n",
      "Epoch: [0][140/5630]\tTime 6.077 (5.677)\tData 3.989 (4.238)\tLoss 5.0894 (5.0807)\tPrec@1 0.000 (1.371)\tPrec@5 6.667 (6.194)\n",
      "Epoch: [0][141/5630]\tTime 6.094 (5.680)\tData 4.053 (4.237)\tLoss 4.8965 (5.0794)\tPrec@1 0.000 (1.362)\tPrec@5 16.667 (6.268)\n",
      "Epoch: [0][142/5630]\tTime 6.768 (5.687)\tData 5.521 (4.246)\tLoss 5.0674 (5.0793)\tPrec@1 6.667 (1.399)\tPrec@5 10.000 (6.294)\n",
      "Epoch: [0][143/5630]\tTime 6.073 (5.690)\tData 4.460 (4.247)\tLoss 4.7765 (5.0772)\tPrec@1 0.000 (1.389)\tPrec@5 16.667 (6.366)\n",
      "Epoch: [0][144/5630]\tTime 6.062 (5.693)\tData 4.088 (4.246)\tLoss 5.1775 (5.0779)\tPrec@1 0.000 (1.379)\tPrec@5 6.667 (6.368)\n",
      "Epoch: [0][145/5630]\tTime 6.084 (5.695)\tData 4.258 (4.246)\tLoss 4.7652 (5.0757)\tPrec@1 3.333 (1.393)\tPrec@5 13.333 (6.416)\n",
      "Epoch: [0][146/5630]\tTime 6.077 (5.698)\tData 4.030 (4.245)\tLoss 4.8340 (5.0741)\tPrec@1 0.000 (1.383)\tPrec@5 3.333 (6.395)\n",
      "Epoch: [0][147/5630]\tTime 6.526 (5.704)\tData 4.055 (4.243)\tLoss 4.8181 (5.0724)\tPrec@1 3.333 (1.396)\tPrec@5 10.000 (6.419)\n",
      "Epoch: [0][148/5630]\tTime 7.523 (5.716)\tData 4.248 (4.243)\tLoss 4.9773 (5.0717)\tPrec@1 3.333 (1.409)\tPrec@5 6.667 (6.421)\n",
      "Epoch: [0][149/5630]\tTime 7.498 (5.728)\tData 4.228 (4.243)\tLoss 5.0478 (5.0716)\tPrec@1 0.000 (1.400)\tPrec@5 0.000 (6.378)\n",
      "Epoch: [0][150/5630]\tTime 7.502 (5.739)\tData 4.038 (4.242)\tLoss 4.9402 (5.0707)\tPrec@1 3.333 (1.413)\tPrec@5 13.333 (6.424)\n",
      "Epoch: [0][151/5630]\tTime 7.493 (5.751)\tData 4.096 (4.241)\tLoss 4.9917 (5.0702)\tPrec@1 3.333 (1.425)\tPrec@5 6.667 (6.425)\n",
      "Epoch: [0][152/5630]\tTime 6.474 (5.756)\tData 3.811 (4.238)\tLoss 4.8268 (5.0686)\tPrec@1 3.333 (1.438)\tPrec@5 10.000 (6.449)\n",
      "Epoch: [0][153/5630]\tTime 6.073 (5.758)\tData 4.027 (4.237)\tLoss 5.0417 (5.0684)\tPrec@1 3.333 (1.450)\tPrec@5 6.667 (6.450)\n",
      "Epoch: [0][154/5630]\tTime 6.060 (5.760)\tData 4.134 (4.236)\tLoss 5.1680 (5.0691)\tPrec@1 0.000 (1.441)\tPrec@5 0.000 (6.409)\n",
      "Epoch: [0][155/5630]\tTime 6.064 (5.762)\tData 4.122 (4.235)\tLoss 4.6840 (5.0666)\tPrec@1 6.667 (1.474)\tPrec@5 20.000 (6.496)\n",
      "Epoch: [0][156/5630]\tTime 6.073 (5.764)\tData 4.267 (4.236)\tLoss 5.2353 (5.0677)\tPrec@1 0.000 (1.465)\tPrec@5 3.333 (6.476)\n",
      "Epoch: [0][157/5630]\tTime 7.461 (5.774)\tData 4.293 (4.236)\tLoss 4.8995 (5.0666)\tPrec@1 0.000 (1.456)\tPrec@5 3.333 (6.456)\n",
      "Epoch: [0][158/5630]\tTime 5.528 (5.773)\tData 4.070 (4.235)\tLoss 5.1264 (5.0670)\tPrec@1 3.333 (1.468)\tPrec@5 6.667 (6.457)\n",
      "Epoch: [0][159/5630]\tTime 6.258 (5.776)\tData 4.035 (4.234)\tLoss 4.9500 (5.0662)\tPrec@1 3.333 (1.479)\tPrec@5 16.667 (6.521)\n",
      "Epoch: [0][160/5630]\tTime 6.285 (5.779)\tData 4.014 (4.232)\tLoss 5.0473 (5.0661)\tPrec@1 0.000 (1.470)\tPrec@5 3.333 (6.501)\n",
      "Epoch: [0][161/5630]\tTime 6.279 (5.782)\tData 3.859 (4.230)\tLoss 5.0905 (5.0663)\tPrec@1 0.000 (1.461)\tPrec@5 3.333 (6.481)\n",
      "Epoch: [0][162/5630]\tTime 7.318 (5.791)\tData 4.252 (4.230)\tLoss 5.0887 (5.0664)\tPrec@1 0.000 (1.452)\tPrec@5 10.000 (6.503)\n",
      "Epoch: [0][163/5630]\tTime 7.992 (5.805)\tData 4.068 (4.229)\tLoss 4.9814 (5.0659)\tPrec@1 0.000 (1.443)\tPrec@5 0.000 (6.463)\n",
      "Epoch: [0][164/5630]\tTime 7.780 (5.817)\tData 4.241 (4.229)\tLoss 4.8490 (5.0646)\tPrec@1 3.333 (1.455)\tPrec@5 20.000 (6.545)\n",
      "Epoch: [0][165/5630]\tTime 6.316 (5.820)\tData 4.530 (4.231)\tLoss 4.9817 (5.0641)\tPrec@1 3.333 (1.466)\tPrec@5 13.333 (6.586)\n",
      "Epoch: [0][166/5630]\tTime 6.342 (5.823)\tData 3.937 (4.229)\tLoss 4.8799 (5.0630)\tPrec@1 0.000 (1.457)\tPrec@5 16.667 (6.647)\n",
      "Epoch: [0][167/5630]\tTime 6.342 (5.826)\tData 4.244 (4.229)\tLoss 5.2777 (5.0643)\tPrec@1 0.000 (1.448)\tPrec@5 3.333 (6.627)\n",
      "Epoch: [0][168/5630]\tTime 6.334 (5.829)\tData 4.377 (4.230)\tLoss 5.1771 (5.0649)\tPrec@1 0.000 (1.440)\tPrec@5 6.667 (6.627)\n",
      "Epoch: [0][169/5630]\tTime 6.339 (5.832)\tData 4.225 (4.230)\tLoss 4.9070 (5.0640)\tPrec@1 0.000 (1.431)\tPrec@5 3.333 (6.608)\n",
      "Epoch: [0][170/5630]\tTime 6.343 (5.835)\tData 3.930 (4.228)\tLoss 4.9859 (5.0635)\tPrec@1 0.000 (1.423)\tPrec@5 3.333 (6.589)\n",
      "Epoch: [0][171/5630]\tTime 6.336 (5.838)\tData 3.978 (4.227)\tLoss 4.8724 (5.0624)\tPrec@1 0.000 (1.415)\tPrec@5 3.333 (6.570)\n",
      "Epoch: [0][172/5630]\tTime 6.338 (5.841)\tData 3.847 (4.225)\tLoss 4.9309 (5.0617)\tPrec@1 0.000 (1.407)\tPrec@5 6.667 (6.570)\n",
      "Epoch: [0][173/5630]\tTime 6.348 (5.844)\tData 4.367 (4.226)\tLoss 5.1122 (5.0620)\tPrec@1 0.000 (1.398)\tPrec@5 0.000 (6.533)\n",
      "Epoch: [0][174/5630]\tTime 6.366 (5.847)\tData 4.019 (4.224)\tLoss 5.1810 (5.0626)\tPrec@1 3.333 (1.410)\tPrec@5 10.000 (6.552)\n",
      "Epoch: [0][175/5630]\tTime 6.343 (5.850)\tData 3.999 (4.223)\tLoss 4.8656 (5.0615)\tPrec@1 10.000 (1.458)\tPrec@5 13.333 (6.591)\n",
      "Epoch: [0][176/5630]\tTime 6.423 (5.853)\tData 4.206 (4.223)\tLoss 4.9873 (5.0611)\tPrec@1 0.000 (1.450)\tPrec@5 0.000 (6.554)\n",
      "Epoch: [0][177/5630]\tTime 7.228 (5.861)\tData 3.837 (4.221)\tLoss 4.8994 (5.0602)\tPrec@1 0.000 (1.442)\tPrec@5 6.667 (6.554)\n",
      "Epoch: [0][178/5630]\tTime 6.462 (5.864)\tData 4.225 (4.221)\tLoss 5.1115 (5.0605)\tPrec@1 0.000 (1.434)\tPrec@5 0.000 (6.518)\n",
      "Epoch: [0][179/5630]\tTime 6.914 (5.870)\tData 3.848 (4.219)\tLoss 5.1296 (5.0609)\tPrec@1 0.000 (1.426)\tPrec@5 0.000 (6.481)\n",
      "Epoch: [0][180/5630]\tTime 6.900 (5.875)\tData 3.960 (4.217)\tLoss 5.0805 (5.0610)\tPrec@1 0.000 (1.418)\tPrec@5 10.000 (6.501)\n",
      "Epoch: [0][181/5630]\tTime 6.901 (5.881)\tData 4.262 (4.218)\tLoss 4.9988 (5.0606)\tPrec@1 3.333 (1.429)\tPrec@5 6.667 (6.502)\n",
      "Epoch: [0][182/5630]\tTime 6.909 (5.887)\tData 4.404 (4.219)\tLoss 5.0029 (5.0603)\tPrec@1 0.000 (1.421)\tPrec@5 6.667 (6.503)\n",
      "Epoch: [0][183/5630]\tTime 6.898 (5.892)\tData 3.963 (4.217)\tLoss 4.9572 (5.0598)\tPrec@1 0.000 (1.413)\tPrec@5 10.000 (6.522)\n",
      "Epoch: [0][184/5630]\tTime 6.886 (5.898)\tData 4.300 (4.218)\tLoss 5.1225 (5.0601)\tPrec@1 0.000 (1.405)\tPrec@5 6.667 (6.523)\n",
      "Epoch: [0][185/5630]\tTime 6.898 (5.903)\tData 3.889 (4.216)\tLoss 4.9485 (5.0595)\tPrec@1 0.000 (1.398)\tPrec@5 0.000 (6.487)\n",
      "Epoch: [0][186/5630]\tTime 6.900 (5.908)\tData 4.470 (4.217)\tLoss 5.1169 (5.0598)\tPrec@1 0.000 (1.390)\tPrec@5 6.667 (6.488)\n",
      "Epoch: [0][187/5630]\tTime 6.902 (5.914)\tData 4.007 (4.216)\tLoss 5.0435 (5.0597)\tPrec@1 3.333 (1.401)\tPrec@5 6.667 (6.489)\n",
      "Epoch: [0][188/5630]\tTime 6.893 (5.919)\tData 5.346 (4.222)\tLoss 5.2024 (5.0605)\tPrec@1 0.000 (1.393)\tPrec@5 13.333 (6.526)\n",
      "Epoch: [0][189/5630]\tTime 6.867 (5.924)\tData 4.229 (4.222)\tLoss 5.2904 (5.0617)\tPrec@1 0.000 (1.386)\tPrec@5 3.333 (6.509)\n",
      "Epoch: [0][190/5630]\tTime 6.088 (5.925)\tData 4.438 (4.223)\tLoss 5.0783 (5.0618)\tPrec@1 0.000 (1.379)\tPrec@5 0.000 (6.475)\n",
      "Epoch: [0][191/5630]\tTime 5.702 (5.923)\tData 3.933 (4.222)\tLoss 4.9581 (5.0612)\tPrec@1 0.000 (1.372)\tPrec@5 3.333 (6.458)\n",
      "Epoch: [0][192/5630]\tTime 5.701 (5.922)\tData 4.022 (4.221)\tLoss 4.9918 (5.0609)\tPrec@1 0.000 (1.364)\tPrec@5 10.000 (6.477)\n",
      "Epoch: [0][193/5630]\tTime 6.868 (5.927)\tData 4.224 (4.221)\tLoss 4.9917 (5.0605)\tPrec@1 3.333 (1.375)\tPrec@5 6.667 (6.478)\n",
      "Epoch: [0][194/5630]\tTime 6.983 (5.933)\tData 3.770 (4.219)\tLoss 4.9486 (5.0599)\tPrec@1 10.000 (1.419)\tPrec@5 10.000 (6.496)\n",
      "Epoch: [0][195/5630]\tTime 6.985 (5.938)\tData 4.288 (4.219)\tLoss 5.0346 (5.0598)\tPrec@1 0.000 (1.412)\tPrec@5 3.333 (6.480)\n",
      "Epoch: [0][196/5630]\tTime 6.979 (5.943)\tData 4.223 (4.219)\tLoss 5.0071 (5.0595)\tPrec@1 3.333 (1.421)\tPrec@5 10.000 (6.497)\n",
      "Epoch: [0][197/5630]\tTime 6.971 (5.948)\tData 3.998 (4.218)\tLoss 4.9979 (5.0592)\tPrec@1 0.000 (1.414)\tPrec@5 0.000 (6.465)\n",
      "Epoch: [0][198/5630]\tTime 6.958 (5.953)\tData 3.970 (4.217)\tLoss 4.9914 (5.0589)\tPrec@1 0.000 (1.407)\tPrec@5 6.667 (6.466)\n",
      "Epoch: [0][199/5630]\tTime 6.964 (5.959)\tData 3.859 (4.215)\tLoss 4.8909 (5.0580)\tPrec@1 3.333 (1.417)\tPrec@5 13.333 (6.500)\n",
      "Epoch: [0][200/5630]\tTime 6.973 (5.964)\tData 3.772 (4.213)\tLoss 5.0010 (5.0578)\tPrec@1 0.000 (1.410)\tPrec@5 13.333 (6.534)\n",
      "Epoch: [0][201/5630]\tTime 6.971 (5.969)\tData 4.276 (4.213)\tLoss 4.8321 (5.0566)\tPrec@1 0.000 (1.403)\tPrec@5 20.000 (6.601)\n",
      "Epoch: [0][202/5630]\tTime 6.954 (5.973)\tData 4.113 (4.212)\tLoss 5.0943 (5.0568)\tPrec@1 0.000 (1.396)\tPrec@5 3.333 (6.585)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][203/5630]\tTime 6.959 (5.978)\tData 4.269 (4.213)\tLoss 5.3556 (5.0583)\tPrec@1 0.000 (1.389)\tPrec@5 3.333 (6.569)\n",
      "Epoch: [0][204/5630]\tTime 6.981 (5.983)\tData 3.994 (4.212)\tLoss 4.8656 (5.0574)\tPrec@1 0.000 (1.382)\tPrec@5 6.667 (6.569)\n",
      "Epoch: [0][205/5630]\tTime 6.961 (5.988)\tData 4.002 (4.211)\tLoss 4.9078 (5.0566)\tPrec@1 0.000 (1.375)\tPrec@5 10.000 (6.586)\n",
      "Epoch: [0][206/5630]\tTime 6.966 (5.993)\tData 3.885 (4.209)\tLoss 4.9323 (5.0560)\tPrec@1 0.000 (1.369)\tPrec@5 3.333 (6.570)\n",
      "Epoch: [0][207/5630]\tTime 6.967 (5.997)\tData 4.335 (4.210)\tLoss 4.9845 (5.0557)\tPrec@1 3.333 (1.378)\tPrec@5 3.333 (6.554)\n",
      "Epoch: [0][208/5630]\tTime 6.970 (6.002)\tData 3.796 (4.208)\tLoss 4.9405 (5.0551)\tPrec@1 0.000 (1.372)\tPrec@5 3.333 (6.539)\n",
      "Epoch: [0][209/5630]\tTime 6.978 (6.007)\tData 4.302 (4.208)\tLoss 5.1261 (5.0555)\tPrec@1 0.000 (1.365)\tPrec@5 3.333 (6.524)\n",
      "Epoch: [0][210/5630]\tTime 6.968 (6.011)\tData 4.266 (4.208)\tLoss 4.9275 (5.0549)\tPrec@1 3.333 (1.374)\tPrec@5 3.333 (6.509)\n",
      "Epoch: [0][211/5630]\tTime 6.964 (6.016)\tData 4.215 (4.208)\tLoss 4.8952 (5.0541)\tPrec@1 0.000 (1.368)\tPrec@5 10.000 (6.525)\n",
      "Epoch: [0][212/5630]\tTime 6.972 (6.020)\tData 4.305 (4.209)\tLoss 4.9642 (5.0537)\tPrec@1 0.000 (1.362)\tPrec@5 3.333 (6.510)\n",
      "Epoch: [0][213/5630]\tTime 6.949 (6.025)\tData 4.174 (4.209)\tLoss 5.0456 (5.0537)\tPrec@1 0.000 (1.355)\tPrec@5 3.333 (6.495)\n",
      "Epoch: [0][214/5630]\tTime 6.979 (6.029)\tData 4.383 (4.209)\tLoss 4.8305 (5.0526)\tPrec@1 10.000 (1.395)\tPrec@5 13.333 (6.527)\n",
      "Epoch: [0][215/5630]\tTime 6.971 (6.033)\tData 4.274 (4.210)\tLoss 4.8593 (5.0517)\tPrec@1 0.000 (1.389)\tPrec@5 6.667 (6.528)\n",
      "Epoch: [0][216/5630]\tTime 6.956 (6.038)\tData 4.059 (4.209)\tLoss 4.8066 (5.0506)\tPrec@1 0.000 (1.382)\tPrec@5 10.000 (6.544)\n",
      "Epoch: [0][217/5630]\tTime 6.973 (6.042)\tData 4.408 (4.210)\tLoss 5.0010 (5.0504)\tPrec@1 0.000 (1.376)\tPrec@5 10.000 (6.560)\n",
      "Epoch: [0][218/5630]\tTime 6.980 (6.046)\tData 4.174 (4.210)\tLoss 5.0373 (5.0503)\tPrec@1 0.000 (1.370)\tPrec@5 6.667 (6.560)\n",
      "Epoch: [0][219/5630]\tTime 6.969 (6.050)\tData 3.908 (4.208)\tLoss 4.7601 (5.0490)\tPrec@1 3.333 (1.379)\tPrec@5 13.333 (6.591)\n",
      "Epoch: [0][220/5630]\tTime 5.800 (6.049)\tData 3.926 (4.207)\tLoss 4.6481 (5.0472)\tPrec@1 6.667 (1.403)\tPrec@5 20.000 (6.652)\n",
      "Epoch: [0][221/5630]\tTime 5.738 (6.048)\tData 3.930 (4.206)\tLoss 4.7222 (5.0457)\tPrec@1 3.333 (1.411)\tPrec@5 13.333 (6.682)\n",
      "Epoch: [0][222/5630]\tTime 5.742 (6.046)\tData 4.223 (4.206)\tLoss 4.8887 (5.0450)\tPrec@1 0.000 (1.405)\tPrec@5 10.000 (6.697)\n",
      "Epoch: [0][223/5630]\tTime 7.736 (6.054)\tData 4.517 (4.207)\tLoss 4.8800 (5.0443)\tPrec@1 3.333 (1.414)\tPrec@5 10.000 (6.711)\n",
      "Epoch: [0][224/5630]\tTime 5.947 (6.053)\tData 4.271 (4.208)\tLoss 4.7794 (5.0431)\tPrec@1 6.667 (1.437)\tPrec@5 20.000 (6.770)\n",
      "You pressed Ctrl+C!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Light\\anaconda3\\envs\\capstone-project\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-project",
   "language": "python",
   "name": "capstone-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
